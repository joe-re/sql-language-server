{
    
    "functions": [
        {
            "name": "!",
            "description": "Function: !\nClass: org.apache.spark.sql.catalyst.expressions.Not\nUsage: ! expr - Logical not.\nExtended Usage:\n    Examples:\n      > SELECT ! true;\n       false\n      > SELECT ! false;\n       true\n      > SELECT ! NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "!=",
            "description": "Function: !=\nUsage: expr1 != expr2 - Returns true if `expr1` is not equal to `expr2`."
        },
        {
            "name": "%",
            "description": "Function: %\nClass: org.apache.spark.sql.catalyst.expressions.Remainder\nUsage: expr1 % expr2 - Returns the remainder after `expr1`/`expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 2 % 1.8;\n       0.2\n      > SELECT MOD(2, 1.8);\n       0.2\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "&",
            "description": "Function: &\nClass: org.apache.spark.sql.catalyst.expressions.BitwiseAnd\nUsage: expr1 & expr2 - Returns the result of bitwise AND of `expr1` and `expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 3 & 5;\n       1\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "*",
            "description": "Function: *\nClass: org.apache.spark.sql.catalyst.expressions.Multiply\nUsage: expr1 * expr2 - Returns `expr1`*`expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 2 * 3;\n       6\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "+",
            "description": "Function: +\nClass: org.apache.spark.sql.catalyst.expressions.Add\nUsage: expr1 + expr2 - Returns `expr1`+`expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 1 + 2;\n       3\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "-",
            "description": "Function: -\nClass: org.apache.spark.sql.catalyst.expressions.Subtract\nUsage: expr1 - expr2 - Returns `expr1`-`expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 2 - 1;\n       1\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "/",
            "description": "Function: /\nClass: org.apache.spark.sql.catalyst.expressions.Divide\nUsage: expr1 / expr2 - Returns `expr1`/`expr2`. It always performs floating point division.\nExtended Usage:\n    Examples:\n      > SELECT 3 / 2;\n       1.5\n      > SELECT 2L / 2L;\n       1.0\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "<",
            "description": "Function: <\nClass: org.apache.spark.sql.catalyst.expressions.LessThan\nUsage: expr1 < expr2 - Returns true if `expr1` is less than `expr2`.\nExtended Usage:\n    Arguments:\n      * expr1, expr2 - the two expressions must be same type or can be casted to a common type,\n          and must be a type that can be ordered. For example, map type is not orderable, so it\n          is not supported. For complex types such array/struct, the data types of fields must\n          be orderable.\n  \n    Examples:\n      > SELECT 1 < 2;\n       true\n      > SELECT 1.1 < '1';\n       false\n      > SELECT to_date('2009-07-30 04:17:52') < to_date('2009-07-30 04:17:52');\n       false\n      > SELECT to_date('2009-07-30 04:17:52') < to_date('2009-08-01 04:17:52');\n       true\n      > SELECT 1 < NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "<=",
            "description": "Function: <=\nClass: org.apache.spark.sql.catalyst.expressions.LessThanOrEqual\nUsage: expr1 <= expr2 - Returns true if `expr1` is less than or equal to `expr2`.\nExtended Usage:\n    Arguments:\n      * expr1, expr2 - the two expressions must be same type or can be casted to a common type,\n          and must be a type that can be ordered. For example, map type is not orderable, so it\n          is not supported. For complex types such array/struct, the data types of fields must\n          be orderable.\n  \n    Examples:\n      > SELECT 2 <= 2;\n       true\n      > SELECT 1.0 <= '1';\n       true\n      > SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-07-30 04:17:52');\n       true\n      > SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-08-01 04:17:52');\n       true\n      > SELECT 1 <= NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "<=>",
            "description": "Function: <=>\nClass: org.apache.spark.sql.catalyst.expressions.EqualNullSafe\nUsage: \n    expr1 <=> expr2 - Returns same result as the EQUAL(=) operator for non-null operands,\n      but returns true if both are null, false if one of the them is null.\n  \nExtended Usage:\n    Arguments:\n      * expr1, expr2 - the two expressions must be same type or can be casted to a common type,\n          and must be a type that can be used in equality comparison. Map type is not supported.\n          For complex types such array/struct, the data types of fields must be orderable.\n  \n    Examples:\n      > SELECT 2 <=> 2;\n       true\n      > SELECT 1 <=> '1';\n       true\n      > SELECT true <=> NULL;\n       false\n      > SELECT NULL <=> NULL;\n       true\n  \n    Since: 1.1.0\n"
        },
        {
            "name": "<>",
            "description": "Function: <>\nUsage: expr1 <> expr2 - Returns true if `expr1` is not equal to `expr2`."
        },
        {
            "name": "=",
            "description": "Function: =\nClass: org.apache.spark.sql.catalyst.expressions.EqualTo\nUsage: expr1 = expr2 - Returns true if `expr1` equals `expr2`, or false otherwise.\nExtended Usage:\n    Arguments:\n      * expr1, expr2 - the two expressions must be same type or can be casted to a common type,\n          and must be a type that can be used in equality comparison. Map type is not supported.\n          For complex types such array/struct, the data types of fields must be orderable.\n  \n    Examples:\n      > SELECT 2 = 2;\n       true\n      > SELECT 1 = '1';\n       true\n      > SELECT true = NULL;\n       NULL\n      > SELECT NULL = NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "==",
            "description": "Function: ==\nClass: org.apache.spark.sql.catalyst.expressions.EqualTo\nUsage: expr1 == expr2 - Returns true if `expr1` equals `expr2`, or false otherwise.\nExtended Usage:\n    Arguments:\n      * expr1, expr2 - the two expressions must be same type or can be casted to a common type,\n          and must be a type that can be used in equality comparison. Map type is not supported.\n          For complex types such array/struct, the data types of fields must be orderable.\n  \n    Examples:\n      > SELECT 2 == 2;\n       true\n      > SELECT 1 == '1';\n       true\n      > SELECT true == NULL;\n       NULL\n      > SELECT NULL == NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": ">",
            "description": "Function: >\nClass: org.apache.spark.sql.catalyst.expressions.GreaterThan\nUsage: expr1 > expr2 - Returns true if `expr1` is greater than `expr2`.\nExtended Usage:\n    Arguments:\n      * expr1, expr2 - the two expressions must be same type or can be casted to a common type,\n          and must be a type that can be ordered. For example, map type is not orderable, so it\n          is not supported. For complex types such array/struct, the data types of fields must\n          be orderable.\n  \n    Examples:\n      > SELECT 2 > 1;\n       true\n      > SELECT 2 > '1.1';\n       true\n      > SELECT to_date('2009-07-30 04:17:52') > to_date('2009-07-30 04:17:52');\n       false\n      > SELECT to_date('2009-07-30 04:17:52') > to_date('2009-08-01 04:17:52');\n       false\n      > SELECT 1 > NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": ">=",
            "description": "Function: >=\nClass: org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual\nUsage: expr1 >= expr2 - Returns true if `expr1` is greater than or equal to `expr2`.\nExtended Usage:\n    Arguments:\n      * expr1, expr2 - the two expressions must be same type or can be casted to a common type,\n          and must be a type that can be ordered. For example, map type is not orderable, so it\n          is not supported. For complex types such array/struct, the data types of fields must\n          be orderable.\n  \n    Examples:\n      > SELECT 2 >= 1;\n       true\n      > SELECT 2.0 >= '2.1';\n       false\n      > SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-07-30 04:17:52');\n       true\n      > SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-08-01 04:17:52');\n       false\n      > SELECT 1 >= NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "^",
            "description": "Function: ^\nClass: org.apache.spark.sql.catalyst.expressions.BitwiseXor\nUsage: expr1 ^ expr2 - Returns the result of bitwise exclusive OR of `expr1` and `expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 3 ^ 5;\n       6\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "abs",
            "description": "Function: abs\nClass: org.apache.spark.sql.catalyst.expressions.Abs\nUsage: abs(expr) - Returns the absolute value of the numeric value.\nExtended Usage:\n    Examples:\n      > SELECT abs(-1);\n       1\n  \n    Since: 1.2.0\n"
        },
        {
            "name": "acos",
            "description": "Function: acos\nClass: org.apache.spark.sql.catalyst.expressions.Acos\nUsage: \n    acos(expr) - Returns the inverse cosine (a.k.a. arc cosine) of `expr`, as if computed by\n      `java.lang.Math.acos`.\n  \nExtended Usage:\n    Examples:\n      > SELECT acos(1);\n       0.0\n      > SELECT acos(2);\n       NaN\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "acosh",
            "description": "Function: acosh\nClass: org.apache.spark.sql.catalyst.expressions.Acosh\nUsage: \n    acosh(expr) - Returns inverse hyperbolic cosine of `expr`.\n  \nExtended Usage:\n    Examples:\n      > SELECT acosh(1);\n       0.0\n      > SELECT acosh(0);\n       NaN\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "add_months",
            "description": "Function: add_months\nClass: org.apache.spark.sql.catalyst.expressions.AddMonths\nUsage: add_months(start_date, num_months) - Returns the date that is `num_months` after `start_date`.\nExtended Usage:\n    Examples:\n      > SELECT add_months('2016-08-31', 1);\n       2016-09-30\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "aggregate",
            "description": "Function: aggregate\nClass: org.apache.spark.sql.catalyst.expressions.ArrayAggregate\nUsage: \n      aggregate(expr, start, merge, finish) - Applies a binary operator to an initial state and all\n      elements in the array, and reduces this to a single state. The final state is converted\n      into the final result by applying a finish function.\n    \nExtended Usage:\n    Examples:\n      > SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x);\n       6\n      > SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x, acc -> acc * 10);\n       60\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "and",
            "description": "Function: and\nClass: org.apache.spark.sql.catalyst.expressions.And\nUsage: expr1 and expr2 - Logical AND.\nExtended Usage:\n    Examples:\n      > SELECT true and true;\n       true\n      > SELECT true and false;\n       false\n      > SELECT true and NULL;\n       NULL\n      > SELECT false and NULL;\n       false\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "any",
            "description": "Function: any\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr\nUsage: any(expr) - Returns true if at least one value of `expr` is true.\nExtended Usage:\n    Examples:\n      > SELECT any(col) FROM VALUES (true), (false), (false) AS tab(col);\n       true\n      > SELECT any(col) FROM VALUES (NULL), (true), (false) AS tab(col);\n       true\n      > SELECT any(col) FROM VALUES (false), (false), (NULL) AS tab(col);\n       false\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "approx_count_distinct",
            "description": "Function: approx_count_distinct\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus\nUsage: \n    approx_count_distinct(expr[, relativeSD]) - Returns the estimated cardinality by HyperLogLog++.\n      `relativeSD` defines the maximum relative standard deviation allowed.\nExtended Usage:\n    Examples:\n      > SELECT approx_count_distinct(col1) FROM VALUES (1), (1), (2), (2), (3) tab(col1);\n       3\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "approx_percentile",
            "description": "Function: approx_percentile\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile\nUsage: \n    approx_percentile(col, percentage [, accuracy]) - Returns the approximate `percentile` of the numeric\n      column `col` which is the smallest value in the ordered `col` values (sorted from least to\n      greatest) such that no more than `percentage` of `col` values is less than the value\n      or equal to that value. The value of percentage must be between 0.0 and 1.0. The `accuracy`\n      parameter (default: 10000) is a positive numeric literal which controls approximation accuracy\n      at the cost of memory. Higher value of `accuracy` yields better accuracy, `1.0/accuracy` is\n      the relative error of the approximation.\n      When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0.\n      In this case, returns the approximate percentile array of column `col` at the given\n      percentage array.\n  \nExtended Usage:\n    Examples:\n      > SELECT approx_percentile(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);\n       [1,1,0]\n      > SELECT approx_percentile(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);\n       7\n  \n    Since: 2.1.0\n"
        },
        {
            "name": "array",
            "description": "Function: array\nClass: org.apache.spark.sql.catalyst.expressions.CreateArray\nUsage: array(expr, ...) - Returns an array with the given elements.\nExtended Usage:\n    Examples:\n      > SELECT array(1, 2, 3);\n       [1,2,3]\n  \n    Since: 1.1.0\n"
        },
        {
            "name": "array_contains",
            "description": "Function: array_contains\nClass: org.apache.spark.sql.catalyst.expressions.ArrayContains\nUsage: array_contains(array, value) - Returns true if the array contains the value.\nExtended Usage:\n    Examples:\n      > SELECT array_contains(array(1, 2, 3), 2);\n       true\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "array_distinct",
            "description": "Function: array_distinct\nClass: org.apache.spark.sql.catalyst.expressions.ArrayDistinct\nUsage: array_distinct(array) - Removes duplicate values from the array.\nExtended Usage:\n    Examples:\n      > SELECT array_distinct(array(1, 2, 3, null, 3));\n       [1,2,3,null]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_except",
            "description": "Function: array_except\nClass: org.apache.spark.sql.catalyst.expressions.ArrayExcept\nUsage: \n  array_except(array1, array2) - Returns an array of the elements in array1 but not in array2,\n    without duplicates.\n  \nExtended Usage:\n    Examples:\n      > SELECT array_except(array(1, 2, 3), array(1, 3, 5));\n       [2]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_intersect",
            "description": "Function: array_intersect\nClass: org.apache.spark.sql.catalyst.expressions.ArrayIntersect\nUsage: \n  array_intersect(array1, array2) - Returns an array of the elements in the intersection of array1 and\n    array2, without duplicates.\n  \nExtended Usage:\n    Examples:\n      > SELECT array_intersect(array(1, 2, 3), array(1, 3, 5));\n       [1,3]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_join",
            "description": "Function: array_join\nClass: org.apache.spark.sql.catalyst.expressions.ArrayJoin\nUsage: \n    array_join(array, delimiter[, nullReplacement]) - Concatenates the elements of the given array\n      using the delimiter and an optional string to replace nulls. If no value is set for\n      nullReplacement, any null value is filtered.\nExtended Usage:\n    Examples:\n      > SELECT array_join(array('hello', 'world'), ' ');\n       hello world\n      > SELECT array_join(array('hello', null ,'world'), ' ');\n       hello world\n      > SELECT array_join(array('hello', null ,'world'), ' ', ',');\n       hello , world\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_max",
            "description": "Function: array_max\nClass: org.apache.spark.sql.catalyst.expressions.ArrayMax\nUsage: array_max(array) - Returns the maximum value in the array. NULL elements are skipped.\nExtended Usage:\n    Examples:\n      > SELECT array_max(array(1, 20, null, 3));\n       20\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_min",
            "description": "Function: array_min\nClass: org.apache.spark.sql.catalyst.expressions.ArrayMin\nUsage: array_min(array) - Returns the minimum value in the array. NULL elements are skipped.\nExtended Usage:\n    Examples:\n      > SELECT array_min(array(1, 20, null, 3));\n       1\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_position",
            "description": "Function: array_position\nClass: org.apache.spark.sql.catalyst.expressions.ArrayPosition\nUsage: \n    array_position(array, element) - Returns the (1-based) index of the first element of the array as long.\n  \nExtended Usage:\n    Examples:\n      > SELECT array_position(array(3, 2, 1), 1);\n       3\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_remove",
            "description": "Function: array_remove\nClass: org.apache.spark.sql.catalyst.expressions.ArrayRemove\nUsage: array_remove(array, element) - Remove all elements that equal to element from array.\nExtended Usage:\n    Examples:\n      > SELECT array_remove(array(1, 2, 3, null, 3), 3);\n       [1,2,null]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_repeat",
            "description": "Function: array_repeat\nClass: org.apache.spark.sql.catalyst.expressions.ArrayRepeat\nUsage: array_repeat(element, count) - Returns the array containing element count times.\nExtended Usage:\n    Examples:\n      > SELECT array_repeat('123', 2);\n       [\"123\",\"123\"]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_sort",
            "description": "Function: array_sort\nClass: org.apache.spark.sql.catalyst.expressions.ArraySort\nUsage: array_sort(expr, func) - Sorts the input array. If func is omitted, sort\n    in ascending order. The elements of the input array must be orderable. Null elements\n    will be placed at the end of the returned array. Since 3.0.0 this function also sorts\n    and returns the array based on the given comparator function. The comparator will\n    take two arguments representing two elements of the array.\n    It returns -1, 0, or 1 as the first element is less than, equal to, or greater\n    than the second element. If the comparator function returns other\n    values (including null), the function will fail and raise an error.\n    \nExtended Usage:\n    Examples:\n      > SELECT array_sort(array(5, 6, 1), (left, right) -> case when left < right then -1 when left > right then 1 else 0 end);\n       [1,5,6]\n      > SELECT array_sort(array('bc', 'ab', 'dc'), (left, right) -> case when left is null and right is null then 0 when left is null then -1 when right is null then 1 when left < right then 1 when left > right then -1 else 0 end);\n       [\"dc\",\"bc\",\"ab\"]\n      > SELECT array_sort(array('b', 'd', null, 'c', 'a'));\n       [\"a\",\"b\",\"c\",\"d\",null]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "array_union",
            "description": "Function: array_union\nClass: org.apache.spark.sql.catalyst.expressions.ArrayUnion\nUsage: \n    array_union(array1, array2) - Returns an array of the elements in the union of array1 and array2,\n      without duplicates.\n  \nExtended Usage:\n    Examples:\n      > SELECT array_union(array(1, 2, 3), array(1, 3, 5));\n       [1,2,3,5]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "arrays_overlap",
            "description": "Function: arrays_overlap\nClass: org.apache.spark.sql.catalyst.expressions.ArraysOverlap\nUsage: arrays_overlap(a1, a2) - Returns true if a1 contains at least a non-null element present also in a2. If the arrays have no common element and they are both non-empty and either of them contains a null element null is returned, false otherwise.\nExtended Usage:\n    Examples:\n      > SELECT arrays_overlap(array(1, 2, 3), array(3, 4, 5));\n       true\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "arrays_zip",
            "description": "Function: arrays_zip\nClass: org.apache.spark.sql.catalyst.expressions.ArraysZip\nUsage: \n    arrays_zip(a1, a2, ...) - Returns a merged array of structs in which the N-th struct contains all\n    N-th values of input arrays.\n  \nExtended Usage:\n    Examples:\n      > SELECT arrays_zip(array(1, 2, 3), array(2, 3, 4));\n       [{\"0\":1,\"1\":2},{\"0\":2,\"1\":3},{\"0\":3,\"1\":4}]\n      > SELECT arrays_zip(array(1, 2), array(2, 3), array(3, 4));\n       [{\"0\":1,\"1\":2,\"2\":3},{\"0\":2,\"1\":3,\"2\":4}]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "ascii",
            "description": "Function: ascii\nClass: org.apache.spark.sql.catalyst.expressions.Ascii\nUsage: ascii(str) - Returns the numeric value of the first character of `str`.\nExtended Usage:\n    Examples:\n      > SELECT ascii('222');\n       50\n      > SELECT ascii(2);\n       50\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "asin",
            "description": "Function: asin\nClass: org.apache.spark.sql.catalyst.expressions.Asin\nUsage: \n    asin(expr) - Returns the inverse sine (a.k.a. arc sine) the arc sin of `expr`,\n      as if computed by `java.lang.Math.asin`.\n  \nExtended Usage:\n    Examples:\n      > SELECT asin(0);\n       0.0\n      > SELECT asin(2);\n       NaN\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "asinh",
            "description": "Function: asinh\nClass: org.apache.spark.sql.catalyst.expressions.Asinh\nUsage: \n    asinh(expr) - Returns inverse hyperbolic sine of `expr`.\n  \nExtended Usage:\n    Examples:\n      > SELECT asinh(0);\n       0.0\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "assert_true",
            "description": "Function: assert_true\nClass: org.apache.spark.sql.catalyst.expressions.AssertTrue\nUsage: assert_true(expr) - Throws an exception if `expr` is not true.\nExtended Usage:\n    Examples:\n      > SELECT assert_true(0 < 1);\n       NULL\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "atan",
            "description": "Function: atan\nClass: org.apache.spark.sql.catalyst.expressions.Atan\nUsage: \n    atan(expr) - Returns the inverse tangent (a.k.a. arc tangent) of `expr`, as if computed by\n      `java.lang.Math.atan`\n  \nExtended Usage:\n    Examples:\n      > SELECT atan(0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "atan2",
            "description": "Function: atan2\nClass: org.apache.spark.sql.catalyst.expressions.Atan2\nUsage: \n    atan2(exprY, exprX) - Returns the angle in radians between the positive x-axis of a plane\n      and the point given by the coordinates (`exprX`, `exprY`), as if computed by\n      `java.lang.Math.atan2`.\n  \nExtended Usage:\n    Arguments:\n      * exprY - coordinate on y-axis\n      * exprX - coordinate on x-axis\n  \n    Examples:\n      > SELECT atan2(0, 0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "atanh",
            "description": "Function: atanh\nClass: org.apache.spark.sql.catalyst.expressions.Atanh\nUsage: \n    atanh(expr) - Returns inverse hyperbolic tangent of `expr`.\n  \nExtended Usage:\n    Examples:\n      > SELECT atanh(0);\n       0.0\n      > SELECT atanh(2);\n       NaN\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "avg",
            "description": "Function: avg\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Average\nUsage: avg(expr) - Returns the mean calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT avg(col) FROM VALUES (1), (2), (3) AS tab(col);\n       2.0\n      > SELECT avg(col) FROM VALUES (1), (2), (NULL) AS tab(col);\n       1.5\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "base64",
            "description": "Function: base64\nClass: org.apache.spark.sql.catalyst.expressions.Base64\nUsage: base64(bin) - Converts the argument from a binary `bin` to a base 64 string.\nExtended Usage:\n    Examples:\n      > SELECT base64('Spark SQL');\n       U3BhcmsgU1FM\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "between",
            "description": "Function: between\nUsage: expr1 [NOT] BETWEEN expr2 AND expr3 - evaluate if `expr1` is [not] in between `expr2` and `expr3`."
        },
        {
            "name": "bigint",
            "description": "Function: bigint\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: bigint(expr) - Casts the value `expr` to the target data type `bigint`.\nExtended Usage:\n    No example/argument for bigint.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "bin",
            "description": "Function: bin\nClass: org.apache.spark.sql.catalyst.expressions.Bin\nUsage: bin(expr) - Returns the string representation of the long value `expr` represented in binary.\nExtended Usage:\n    Examples:\n      > SELECT bin(13);\n       1101\n      > SELECT bin(-13);\n       1111111111111111111111111111111111111111111111111111111111110011\n      > SELECT bin(13.3);\n       1101\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "binary",
            "description": "Function: binary\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: binary(expr) - Casts the value `expr` to the target data type `binary`.\nExtended Usage:\n    No example/argument for binary.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "bit_and",
            "description": "Function: bit_and\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg\nUsage: bit_and(expr) - Returns the bitwise AND of all non-null input values, or null if none.\nExtended Usage:\n    Examples:\n      > SELECT bit_and(col) FROM VALUES (3), (5) AS tab(col);\n       1\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "bit_count",
            "description": "Function: bit_count\nClass: org.apache.spark.sql.catalyst.expressions.BitwiseCount\nUsage: bit_count(expr) - Returns the number of bits that are set in the argument expr as an unsigned 64-bit integer, or NULL if the argument is NULL.\nExtended Usage:\n    Examples:\n      > SELECT bit_count(0);\n       0\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "bit_length",
            "description": "Function: bit_length\nClass: org.apache.spark.sql.catalyst.expressions.BitLength\nUsage: bit_length(expr) - Returns the bit length of string data or number of bits of binary data.\nExtended Usage:\n    Examples:\n      > SELECT bit_length('Spark SQL');\n       72\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "bit_or",
            "description": "Function: bit_or\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg\nUsage: bit_or(expr) - Returns the bitwise OR of all non-null input values, or null if none.\nExtended Usage:\n    Examples:\n      > SELECT bit_or(col) FROM VALUES (3), (5) AS tab(col);\n       7\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "bit_xor",
            "description": "Function: bit_xor\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg\nUsage: bit_xor(expr) - Returns the bitwise XOR of all non-null input values, or null if none.\nExtended Usage:\n    Examples:\n      > SELECT bit_xor(col) FROM VALUES (3), (5) AS tab(col);\n       6\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "bool_and",
            "description": "Function: bool_and\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd\nUsage: bool_and(expr) - Returns true if all values of `expr` are true.\nExtended Usage:\n    Examples:\n      > SELECT bool_and(col) FROM VALUES (true), (true), (true) AS tab(col);\n       true\n      > SELECT bool_and(col) FROM VALUES (NULL), (true), (true) AS tab(col);\n       true\n      > SELECT bool_and(col) FROM VALUES (true), (false), (true) AS tab(col);\n       false\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "bool_or",
            "description": "Function: bool_or\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr\nUsage: bool_or(expr) - Returns true if at least one value of `expr` is true.\nExtended Usage:\n    Examples:\n      > SELECT bool_or(col) FROM VALUES (true), (false), (false) AS tab(col);\n       true\n      > SELECT bool_or(col) FROM VALUES (NULL), (true), (false) AS tab(col);\n       true\n      > SELECT bool_or(col) FROM VALUES (false), (false), (NULL) AS tab(col);\n       false\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "boolean",
            "description": "Function: boolean\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: boolean(expr) - Casts the value `expr` to the target data type `boolean`.\nExtended Usage:\n    No example/argument for boolean.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "bround",
            "description": "Function: bround\nClass: org.apache.spark.sql.catalyst.expressions.BRound\nUsage: bround(expr, d) - Returns `expr` rounded to `d` decimal places using HALF_EVEN rounding mode.\nExtended Usage:\n    Examples:\n      > SELECT bround(2.5, 0);\n       2\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "cardinality",
            "description": "Function: cardinality\nClass: org.apache.spark.sql.catalyst.expressions.Size\nUsage: \n    cardinality(expr) - Returns the size of an array or a map.\n    The function returns null for null input if spark.sql.legacy.sizeOfNull is set to false or\n    spark.sql.ansi.enabled is set to true. Otherwise, the function returns -1 for null input.\n    With the default settings, the function returns -1 for null input.\n  \nExtended Usage:\n    Examples:\n      > SELECT cardinality(array('b', 'd', 'c', 'a'));\n       4\n      > SELECT cardinality(map('a', 1, 'b', 2));\n       2\n      > SELECT cardinality(NULL);\n       -1\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "case",
            "description": "Function: case\nUsage: CASE expr1 WHEN expr2 THEN expr3 [WHEN expr4 THEN expr5]* [ELSE expr6] END - When `expr1` = `expr2`, returns `expr3`; when `expr1` = `expr4`, return `expr5`; else return `expr6`."
        },
        {
            "name": "cast",
            "description": "Function: cast\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: cast(expr AS type) - Casts the value `expr` to the target data type `type`.\nExtended Usage:\n    Examples:\n      > SELECT cast('10' as int);\n       10\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "cbrt",
            "description": "Function: cbrt\nClass: org.apache.spark.sql.catalyst.expressions.Cbrt\nUsage: cbrt(expr) - Returns the cube root of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT cbrt(27.0);\n       3.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "ceil",
            "description": "Function: ceil\nClass: org.apache.spark.sql.catalyst.expressions.Ceil\nUsage: ceil(expr) - Returns the smallest integer not smaller than `expr`.\nExtended Usage:\n    Examples:\n      > SELECT ceil(-0.1);\n       0\n      > SELECT ceil(5);\n       5\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "ceiling",
            "description": "Function: ceiling\nClass: org.apache.spark.sql.catalyst.expressions.Ceil\nUsage: ceiling(expr) - Returns the smallest integer not smaller than `expr`.\nExtended Usage:\n    Examples:\n      > SELECT ceiling(-0.1);\n       0\n      > SELECT ceiling(5);\n       5\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "char",
            "description": "Function: char\nClass: org.apache.spark.sql.catalyst.expressions.Chr\nUsage: char(expr) - Returns the ASCII character having the binary equivalent to `expr`. If n is larger than 256 the result is equivalent to chr(n % 256)\nExtended Usage:\n    Examples:\n      > SELECT char(65);\n       A\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "char_length",
            "description": "Function: char_length\nClass: org.apache.spark.sql.catalyst.expressions.Length\nUsage: char_length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.\nExtended Usage:\n    Examples:\n      > SELECT char_length('Spark SQL ');\n       10\n      > SELECT CHAR_LENGTH('Spark SQL ');\n       10\n      > SELECT CHARACTER_LENGTH('Spark SQL ');\n       10\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "character_length",
            "description": "Function: character_length\nClass: org.apache.spark.sql.catalyst.expressions.Length\nUsage: character_length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.\nExtended Usage:\n    Examples:\n      > SELECT character_length('Spark SQL ');\n       10\n      > SELECT CHAR_LENGTH('Spark SQL ');\n       10\n      > SELECT CHARACTER_LENGTH('Spark SQL ');\n       10\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "chr",
            "description": "Function: chr\nClass: org.apache.spark.sql.catalyst.expressions.Chr\nUsage: chr(expr) - Returns the ASCII character having the binary equivalent to `expr`. If n is larger than 256 the result is equivalent to chr(n % 256)\nExtended Usage:\n    Examples:\n      > SELECT chr(65);\n       A\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "coalesce",
            "description": "Function: coalesce\nClass: org.apache.spark.sql.catalyst.expressions.Coalesce\nUsage: coalesce(expr1, expr2, ...) - Returns the first non-null argument if exists. Otherwise, null.\nExtended Usage:\n    Examples:\n      > SELECT coalesce(NULL, 1, NULL);\n       1\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "collect_list",
            "description": "Function: collect_list\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.CollectList\nUsage: collect_list(expr) - Collects and returns a list of non-unique elements.\nExtended Usage:\n    Examples:\n      > SELECT collect_list(col) FROM VALUES (1), (2), (1) AS tab(col);\n       [1,2,1]\n  \n    Note:\n      The function is non-deterministic because the order of collected results depends\n    on the order of the rows which may be non-deterministic after a shuffle.\n\n    Since: 2.0.0\n"
        },
        {
            "name": "collect_set",
            "description": "Function: collect_set\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet\nUsage: collect_set(expr) - Collects and returns a set of unique elements.\nExtended Usage:\n    Examples:\n      > SELECT collect_set(col) FROM VALUES (1), (2), (1) AS tab(col);\n       [1,2]\n  \n    Note:\n      The function is non-deterministic because the order of collected results depends\n    on the order of the rows which may be non-deterministic after a shuffle.\n\n    Since: 2.0.0\n"
        },
        {
            "name": "concat",
            "description": "Function: concat\nClass: org.apache.spark.sql.catalyst.expressions.Concat\nUsage: concat(col1, col2, ..., colN) - Returns the concatenation of col1, col2, ..., colN.\nExtended Usage:\n    Examples:\n      > SELECT concat('Spark', 'SQL');\n       SparkSQL\n      > SELECT concat(array(1, 2, 3), array(4, 5), array(6));\n       [1,2,3,4,5,6]\n  \n    Note:\n      Concat logic for arrays is available since 2.4.0.\n\n    Since: 1.5.0\n"
        },
        {
            "name": "concat_ws",
            "description": "Function: concat_ws\nClass: org.apache.spark.sql.catalyst.expressions.ConcatWs\nUsage: concat_ws(sep[, str | array(str)]+) - Returns the concatenation of the strings separated by `sep`.\nExtended Usage:\n    Examples:\n      > SELECT concat_ws(' ', 'Spark', 'SQL');\n        Spark SQL\n      > SELECT concat_ws('s');\n\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "conv",
            "description": "Function: conv\nClass: org.apache.spark.sql.catalyst.expressions.Conv\nUsage: conv(num, from_base, to_base) - Convert `num` from `from_base` to `to_base`.\nExtended Usage:\n    Examples:\n      > SELECT conv('100', 2, 10);\n       4\n      > SELECT conv(-10, 16, -10);\n       -16\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "corr",
            "description": "Function: corr\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Corr\nUsage: corr(expr1, expr2) - Returns Pearson coefficient of correlation between a set of number pairs.\nExtended Usage:\n    Examples:\n      > SELECT corr(c1, c2) FROM VALUES (3, 2), (3, 3), (6, 4) as tab(c1, c2);\n       0.8660254037844387\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "cos",
            "description": "Function: cos\nClass: org.apache.spark.sql.catalyst.expressions.Cos\nUsage: \n    cos(expr) - Returns the cosine of `expr`, as if computed by\n      `java.lang.Math.cos`.\n  \nExtended Usage:\n    Arguments:\n      * expr - angle in radians\n  \n    Examples:\n      > SELECT cos(0);\n       1.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "cosh",
            "description": "Function: cosh\nClass: org.apache.spark.sql.catalyst.expressions.Cosh\nUsage: \n      cosh(expr) - Returns the hyperbolic cosine of `expr`, as if computed by\n        `java.lang.Math.cosh`.\n  \nExtended Usage:\n    Arguments:\n      * expr - hyperbolic angle\n  \n    Examples:\n      > SELECT cosh(0);\n       1.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "cot",
            "description": "Function: cot\nClass: org.apache.spark.sql.catalyst.expressions.Cot\nUsage: \n    cot(expr) - Returns the cotangent of `expr`, as if computed by `1/java.lang.Math.cot`.\n  \nExtended Usage:\n    Arguments:\n      * expr - angle in radians\n  \n    Examples:\n      > SELECT cot(1);\n       0.6420926159343306\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "count",
            "description": "Function: count\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Count\nUsage: \n    count(*) - Returns the total number of retrieved rows, including rows containing null.\n\n    count(expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are all non-null.\n\n    count(DISTINCT expr[, expr...]) - Returns the number of rows for which the supplied expression(s) are unique and non-null.\n  \nExtended Usage:\n    Examples:\n      > SELECT count(*) FROM VALUES (NULL), (5), (5), (20) AS tab(col);\n       4\n      > SELECT count(col) FROM VALUES (NULL), (5), (5), (20) AS tab(col);\n       3\n      > SELECT count(DISTINCT col) FROM VALUES (NULL), (5), (5), (10) AS tab(col);\n       2\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "count_if",
            "description": "Function: count_if\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.CountIf\nUsage: \n    count_if(expr) - Returns the number of `TRUE` values for the expression.\n  \nExtended Usage:\n    Examples:\n      > SELECT count_if(col % 2 = 0) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);\n       2\n      > SELECT count_if(col IS NULL) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);\n       1\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "count_min_sketch",
            "description": "Function: count_min_sketch\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg\nUsage: \n    count_min_sketch(col, eps, confidence, seed) - Returns a count-min sketch of a column with the given esp,\n      confidence and seed. The result is an array of bytes, which can be deserialized to a\n      `CountMinSketch` before usage. Count-min sketch is a probabilistic data structure used for\n      cardinality estimation using sub-linear space.\n  \nExtended Usage:\n    Examples:\n      > SELECT hex(count_min_sketch(col, 0.5d, 0.5d, 1)) FROM VALUES (1), (2), (1) AS tab(col);\n       0000000100000000000000030000000100000004000000005D8D6AB90000000000000000000000000000000200000000000000010000000000000000\n  \n    Since: 2.2.0\n"
        },
        {
            "name": "covar_pop",
            "description": "Function: covar_pop\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation\nUsage: covar_pop(expr1, expr2) - Returns the population covariance of a set of number pairs.\nExtended Usage:\n    Examples:\n      > SELECT covar_pop(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);\n       0.6666666666666666\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "covar_samp",
            "description": "Function: covar_samp\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.CovSample\nUsage: covar_samp(expr1, expr2) - Returns the sample covariance of a set of number pairs.\nExtended Usage:\n    Examples:\n      > SELECT covar_samp(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);\n       1.0\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "crc32",
            "description": "Function: crc32\nClass: org.apache.spark.sql.catalyst.expressions.Crc32\nUsage: crc32(expr) - Returns a cyclic redundancy check value of the `expr` as a bigint.\nExtended Usage:\n    Examples:\n      > SELECT crc32('Spark');\n       1557323817\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "cube",
            "description": "Function: cube\nClass: org.apache.spark.sql.catalyst.expressions.Cube\nUsage: \n    cube([col1[, col2 ..]]) - create a multi-dimensional cube using the specified columns\n      so that we can run aggregation on them.\n  \nExtended Usage:\n    Examples:\n      > SELECT name, age, count(*) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY cube(name, age);\n        Bob\t5\t1\n        Alice\t2\t1\n        Alice\tNULL\t1\n        NULL\t2\t1\n        NULL\tNULL\t2\n        Bob\tNULL\t1\n        NULL\t5\t1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "cume_dist",
            "description": "Function: cume_dist\nClass: org.apache.spark.sql.catalyst.expressions.CumeDist\nUsage: \n    cume_dist() - Computes the position of a value relative to all values in the partition.\n  \nExtended Usage:\n    Examples:\n      > SELECT a, b, cume_dist() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t0.6666666666666666\n       A1\t1\t0.6666666666666666\n       A1\t2\t1.0\n       A2\t3\t1.0\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "current_catalog",
            "description": "Function: current_catalog\nClass: org.apache.spark.sql.catalyst.expressions.CurrentCatalog\nUsage: current_catalog() - Returns the current catalog.\nExtended Usage:\n    Examples:\n      > SELECT current_catalog();\n       spark_catalog\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "current_database",
            "description": "Function: current_database\nClass: org.apache.spark.sql.catalyst.expressions.CurrentDatabase\nUsage: current_database() - Returns the current database.\nExtended Usage:\n    Examples:\n      > SELECT current_database();\n       default\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "current_date",
            "description": "Function: current_date\nClass: org.apache.spark.sql.catalyst.expressions.CurrentDate\nUsage: \n    current_date() - Returns the current date at the start of query evaluation. All calls of current_date within the same query return the same value.\n\n    current_date - Returns the current date at the start of query evaluation.\n  \nExtended Usage:\n    Examples:\n      > SELECT current_date();\n       2020-04-25\n      > SELECT current_date;\n       2020-04-25\n  \n    Note:\n      The syntax without braces has been supported since 2.0.1.\n\n    Since: 1.5.0\n"
        },
        {
            "name": "current_timestamp",
            "description": "Function: current_timestamp\nClass: org.apache.spark.sql.catalyst.expressions.CurrentTimestamp\nUsage: \n    current_timestamp() - Returns the current timestamp at the start of query evaluation. All calls of current_timestamp within the same query return the same value.\n\n    current_timestamp - Returns the current timestamp at the start of query evaluation.\n  \nExtended Usage:\n    Examples:\n      > SELECT current_timestamp();\n       2020-04-25 15:49:11.914\n      > SELECT current_timestamp;\n       2020-04-25 15:49:11.914\n  \n    Note:\n      The syntax without braces has been supported since 2.0.1.\n\n    Since: 1.5.0\n"
        },
        {
            "name": "current_timezone",
            "description": "Function: current_timezone\nClass: org.apache.spark.sql.catalyst.expressions.CurrentTimeZone\nUsage: current_timezone() - Returns the current session local timezone.\nExtended Usage:\n    Examples:\n      > SELECT current_timezone();\n       Asia/Shanghai\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "date",
            "description": "Function: date\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: date(expr) - Casts the value `expr` to the target data type `date`.\nExtended Usage:\n    No example/argument for date.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "date_add",
            "description": "Function: date_add\nClass: org.apache.spark.sql.catalyst.expressions.DateAdd\nUsage: date_add(start_date, num_days) - Returns the date that is `num_days` after `start_date`.\nExtended Usage:\n    Examples:\n      > SELECT date_add('2016-07-30', 1);\n       2016-07-31\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "date_format",
            "description": "Function: date_format\nClass: org.apache.spark.sql.catalyst.expressions.DateFormatClass\nUsage: date_format(timestamp, fmt) - Converts `timestamp` to a value of string in the format specified by the date format `fmt`.\nExtended Usage:\n    Arguments:\n      * timestamp - A date/timestamp or string to be converted to the given format.\n      * fmt - Date/time format pattern to follow. See <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\">Datetime Patterns</a> for valid date\n              and time format patterns.\n  \n    Examples:\n      > SELECT date_format('2016-04-08', 'y');\n       2016\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "date_from_unix_date",
            "description": "Function: date_from_unix_date\nClass: org.apache.spark.sql.catalyst.expressions.DateFromUnixDate\nUsage: date_from_unix_date(days) - Create date from the number of days since 1970-01-01.\nExtended Usage:\n    Examples:\n      > SELECT date_from_unix_date(1);\n       1970-01-02\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "date_part",
            "description": "Function: date_part\nClass: org.apache.spark.sql.catalyst.expressions.DatePart\nUsage: date_part(field, source) - Extracts a part of the date/timestamp or interval source.\nExtended Usage:\n    Arguments:\n      * field - selects which part of the source should be extracted, and supported string values are as same as the fields of the equivalent function `EXTRACT`.\n      * source - a date/timestamp or interval column from where `field` should be extracted\n  \n    Examples:\n      > SELECT date_part('YEAR', TIMESTAMP '2019-08-12 01:00:00.123456');\n       2019\n      > SELECT date_part('week', timestamp'2019-08-12 01:00:00.123456');\n       33\n      > SELECT date_part('doy', DATE'2019-08-12');\n       224\n      > SELECT date_part('SECONDS', timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT date_part('days', interval 1 year 10 months 5 days);\n       5\n      > SELECT date_part('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);\n       30.001001\n  \n    Note:\n      The date_part function is equivalent to the SQL-standard function `EXTRACT(field FROM source)`\n\n    Since: 3.0.0\n"
        },
        {
            "name": "date_sub",
            "description": "Function: date_sub\nClass: org.apache.spark.sql.catalyst.expressions.DateSub\nUsage: date_sub(start_date, num_days) - Returns the date that is `num_days` before `start_date`.\nExtended Usage:\n    Examples:\n      > SELECT date_sub('2016-07-30', 1);\n       2016-07-29\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "date_trunc",
            "description": "Function: date_trunc\nClass: org.apache.spark.sql.catalyst.expressions.TruncTimestamp\nUsage: \n    date_trunc(fmt, ts) - Returns timestamp `ts` truncated to the unit specified by the format model `fmt`.\n  \nExtended Usage:\n    Arguments:\n      * fmt - the format representing the unit to be truncated to\n          - \"YEAR\", \"YYYY\", \"YY\" - truncate to the first date of the year that the `ts` falls in, the time part will be zero out\n          - \"QUARTER\" - truncate to the first date of the quarter that the `ts` falls in, the time part will be zero out\n          - \"MONTH\", \"MM\", \"MON\" - truncate to the first date of the month that the `ts` falls in, the time part will be zero out\n          - \"WEEK\" - truncate to the Monday of the week that the `ts` falls in, the time part will be zero out\n          - \"DAY\", \"DD\" - zero out the time part\n          - \"HOUR\" - zero out the minute and second with fraction part\n          - \"MINUTE\"- zero out the second with fraction part\n          - \"SECOND\" -  zero out the second fraction part\n          - \"MILLISECOND\" - zero out the microseconds\n          - \"MICROSECOND\" - everything remains\n      * ts - datetime value or valid timestamp string\n  \n    Examples:\n      > SELECT date_trunc('YEAR', '2015-03-05T09:32:05.359');\n       2015-01-01 00:00:00\n      > SELECT date_trunc('MM', '2015-03-05T09:32:05.359');\n       2015-03-01 00:00:00\n      > SELECT date_trunc('DD', '2015-03-05T09:32:05.359');\n       2015-03-05 00:00:00\n      > SELECT date_trunc('HOUR', '2015-03-05T09:32:05.359');\n       2015-03-05 09:00:00\n      > SELECT date_trunc('MILLISECOND', '2015-03-05T09:32:05.123456');\n       2015-03-05 09:32:05.123\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "datediff",
            "description": "Function: datediff\nClass: org.apache.spark.sql.catalyst.expressions.DateDiff\nUsage: datediff(endDate, startDate) - Returns the number of days from `startDate` to `endDate`.\nExtended Usage:\n    Examples:\n      > SELECT datediff('2009-07-31', '2009-07-30');\n       1\n\n      > SELECT datediff('2009-07-30', '2009-07-31');\n       -1\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "day",
            "description": "Function: day\nClass: org.apache.spark.sql.catalyst.expressions.DayOfMonth\nUsage: day(date) - Returns the day of month of the date/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT day('2009-07-30');\n       30\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "dayofmonth",
            "description": "Function: dayofmonth\nClass: org.apache.spark.sql.catalyst.expressions.DayOfMonth\nUsage: dayofmonth(date) - Returns the day of month of the date/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT dayofmonth('2009-07-30');\n       30\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "dayofweek",
            "description": "Function: dayofweek\nClass: org.apache.spark.sql.catalyst.expressions.DayOfWeek\nUsage: dayofweek(date) - Returns the day of the week for date/timestamp (1 = Sunday, 2 = Monday, ..., 7 = Saturday).\nExtended Usage:\n    Examples:\n      > SELECT dayofweek('2009-07-30');\n       5\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "dayofyear",
            "description": "Function: dayofyear\nClass: org.apache.spark.sql.catalyst.expressions.DayOfYear\nUsage: dayofyear(date) - Returns the day of year of the date/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT dayofyear('2016-04-09');\n       100\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "decimal",
            "description": "Function: decimal\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: decimal(expr) - Casts the value `expr` to the target data type `decimal`.\nExtended Usage:\n    No example/argument for decimal.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "decode",
            "description": "Function: decode\nClass: org.apache.spark.sql.catalyst.expressions.Decode\nUsage: decode(bin, charset) - Decodes the first argument using the second argument character set.\nExtended Usage:\n    Examples:\n      > SELECT decode(encode('abc', 'utf-8'), 'utf-8');\n       abc\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "degrees",
            "description": "Function: degrees\nClass: org.apache.spark.sql.catalyst.expressions.ToDegrees\nUsage: degrees(expr) - Converts radians to degrees.\nExtended Usage:\n    Arguments:\n      * expr - angle in radians\n  \n    Examples:\n      > SELECT degrees(3.141592653589793);\n       180.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "dense_rank",
            "description": "Function: dense_rank\nClass: org.apache.spark.sql.catalyst.expressions.DenseRank\nUsage: \n    dense_rank() - Computes the rank of a value in a group of values. The result is one plus the\n      previously assigned rank value. Unlike the function rank, dense_rank will not produce gaps\n      in the ranking sequence.\n  \nExtended Usage:\n    Arguments:\n      * children - this is to base the rank on; a change in the value of one the children will\n          trigger a change in rank. This is an internal parameter and will be assigned by the\n          Analyser.\n  \n    Examples:\n      > SELECT a, b, dense_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t1\n       A1\t1\t1\n       A1\t2\t2\n       A2\t3\t1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "div",
            "description": "Function: div\nClass: org.apache.spark.sql.catalyst.expressions.IntegralDivide\nUsage: expr1 div expr2 - Divide `expr1` by `expr2`. It returns NULL if an operand is NULL or `expr2` is 0. The result is casted to long.\nExtended Usage:\n    Examples:\n      > SELECT 3 div 2;\n       1\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "double",
            "description": "Function: double\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: double(expr) - Casts the value `expr` to the target data type `double`.\nExtended Usage:\n    No example/argument for double.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "e",
            "description": "Function: e\nClass: org.apache.spark.sql.catalyst.expressions.EulerNumber\nUsage: e() - Returns Euler's number, e.\nExtended Usage:\n    Examples:\n      > SELECT e();\n       2.718281828459045\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "element_at",
            "description": "Function: element_at\nClass: org.apache.spark.sql.catalyst.expressions.ElementAt\nUsage: \n    element_at(array, index) - Returns element of array at given (1-based) index. If index < 0,\n      accesses elements from the last to the first. The function returns NULL\n      if the index exceeds the length of the array and `spark.sql.ansi.enabled` is set to false.\n      If `spark.sql.ansi.enabled` is set to true, it throws ArrayIndexOutOfBoundsException\n      for invalid indices.\n\n    element_at(map, key) - Returns value for given key. The function returns NULL\n      if the key is not contained in the map and `spark.sql.ansi.enabled` is set to false.\n      If `spark.sql.ansi.enabled` is set to true, it throws NoSuchElementException instead.\n  \nExtended Usage:\n    Examples:\n      > SELECT element_at(array(1, 2, 3), 2);\n       2\n      > SELECT element_at(map(1, 'a', 2, 'b'), 2);\n       b\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "elt",
            "description": "Function: elt\nClass: org.apache.spark.sql.catalyst.expressions.Elt\nUsage: \n    elt(n, input1, input2, ...) - Returns the `n`-th input, e.g., returns `input2` when `n` is 2.\n    The function returns NULL if the index exceeds the length of the array\n    and `spark.sql.ansi.enabled` is set to false. If `spark.sql.ansi.enabled` is set to true,\n    it throws ArrayIndexOutOfBoundsException for invalid indices.\n  \nExtended Usage:\n    Examples:\n      > SELECT elt(1, 'scala', 'java');\n       scala\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "encode",
            "description": "Function: encode\nClass: org.apache.spark.sql.catalyst.expressions.Encode\nUsage: encode(str, charset) - Encodes the first argument using the second argument character set.\nExtended Usage:\n    Examples:\n      > SELECT encode('abc', 'utf-8');\n       abc\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "every",
            "description": "Function: every\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd\nUsage: every(expr) - Returns true if all values of `expr` are true.\nExtended Usage:\n    Examples:\n      > SELECT every(col) FROM VALUES (true), (true), (true) AS tab(col);\n       true\n      > SELECT every(col) FROM VALUES (NULL), (true), (true) AS tab(col);\n       true\n      > SELECT every(col) FROM VALUES (true), (false), (true) AS tab(col);\n       false\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "exists",
            "description": "Function: exists\nClass: org.apache.spark.sql.catalyst.expressions.ArrayExists\nUsage: exists(expr, pred) - Tests whether a predicate holds for one or more elements in the array.\nExtended Usage:\n    Examples:\n      > SELECT exists(array(1, 2, 3), x -> x % 2 == 0);\n       true\n      > SELECT exists(array(1, 2, 3), x -> x % 2 == 10);\n       false\n      > SELECT exists(array(1, null, 3), x -> x % 2 == 0);\n       NULL\n      > SELECT exists(array(0, null, 2, 3, null), x -> x IS NULL);\n       true\n      > SELECT exists(array(1, 2, 3), x -> x IS NULL);\n       false\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "exp",
            "description": "Function: exp\nClass: org.apache.spark.sql.catalyst.expressions.Exp\nUsage: exp(expr) - Returns e to the power of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT exp(0);\n       1.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "explode",
            "description": "Function: explode\nClass: org.apache.spark.sql.catalyst.expressions.Explode\nUsage: explode(expr) - Separates the elements of array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns. Unless specified otherwise, uses the default column name `col` for elements of the array or `key` and `value` for the elements of the map.\nExtended Usage:\n    Examples:\n      > SELECT explode(array(10, 20));\n       10\n       20\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "explode_outer",
            "description": "Function: explode_outer\nClass: org.apache.spark.sql.catalyst.expressions.Explode\nUsage: explode_outer(expr) - Separates the elements of array `expr` into multiple rows, or the elements of map `expr` into multiple rows and columns. Unless specified otherwise, uses the default column name `col` for elements of the array or `key` and `value` for the elements of the map.\nExtended Usage:\n    Examples:\n      > SELECT explode_outer(array(10, 20));\n       10\n       20\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "expm1",
            "description": "Function: expm1\nClass: org.apache.spark.sql.catalyst.expressions.Expm1\nUsage: expm1(expr) - Returns exp(`expr`) - 1.\nExtended Usage:\n    Examples:\n      > SELECT expm1(0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "extract",
            "description": "Function: extract\nClass: org.apache.spark.sql.catalyst.expressions.Extract\nUsage: extract(field FROM source) - Extracts a part of the date/timestamp or interval source.\nExtended Usage:\n    Arguments:\n      * field - selects which part of the source should be extracted\n          - Supported string values of `field` for dates and timestamps are(case insensitive):\n              - \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n              - \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n              - \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n              - \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n              - \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n              - \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n              - \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n              - \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n              - \"DOY\" - the day of the year (1 - 365/366)\n              - \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n              - \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n              - \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts\n          - Supported string values of `field` for interval(which consists of `months`, `days`, `microseconds`) are(case insensitive):\n              - \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the total `months` / 12\n              - \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the total `months` % 12\n              - \"DAY\", (\"D\", \"DAYS\") - the `days` part of interval\n              - \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - how many hours the `microseconds` contains\n              - \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - how many minutes left after taking hours from `microseconds`\n              - \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - how many second with fractions left after taking hours and minutes from `microseconds`\n      * source - a date/timestamp or interval column from where `field` should be extracted\n  \n    Examples:\n      > SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');\n       2019\n      > SELECT extract(week FROM timestamp'2019-08-12 01:00:00.123456');\n       33\n      > SELECT extract(doy FROM DATE'2019-08-12');\n       224\n      > SELECT extract(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');\n       1.000001\n      > SELECT extract(days FROM interval 1 year 10 months 5 days);\n       5\n      > SELECT extract(seconds FROM interval 5 hours 30 seconds 1 milliseconds 1 microseconds);\n       30.001001\n  \n    Note:\n      The extract function is equivalent to `date_part(field, source)`.\n\n    Since: 3.0.0\n"
        },
        {
            "name": "factorial",
            "description": "Function: factorial\nClass: org.apache.spark.sql.catalyst.expressions.Factorial\nUsage: factorial(expr) - Returns the factorial of `expr`. `expr` is [0..20]. Otherwise, null.\nExtended Usage:\n    Examples:\n      > SELECT factorial(5);\n       120\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "filter",
            "description": "Function: filter\nClass: org.apache.spark.sql.catalyst.expressions.ArrayFilter\nUsage: filter(expr, func) - Filters the input array using the given predicate.\nExtended Usage:\n    Examples:\n      > SELECT filter(array(1, 2, 3), x -> x % 2 == 1);\n       [1,3]\n      > SELECT filter(array(0, 2, 3), (x, i) -> x > i);\n       [2,3]\n      > SELECT filter(array(0, null, 2, 3, null), x -> x IS NOT NULL);\n       [0,2,3]\n  \n    Note:\n      The inner function may use the index argument since 3.0.0.\n\n    Since: 2.4.0\n"
        },
        {
            "name": "find_in_set",
            "description": "Function: find_in_set\nClass: org.apache.spark.sql.catalyst.expressions.FindInSet\nUsage: \n    find_in_set(str, str_array) - Returns the index (1-based) of the given string (`str`) in the comma-delimited list (`str_array`).\n      Returns 0, if the string was not found or if the given string (`str`) contains a comma.\n  \nExtended Usage:\n    Examples:\n      > SELECT find_in_set('ab','abc,b,ab,c,def');\n       3\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "first",
            "description": "Function: first\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.First\nUsage: \n    first(expr[, isIgnoreNull]) - Returns the first value of `expr` for a group of rows.\n      If `isIgnoreNull` is true, returns only non-null values.\nExtended Usage:\n    Examples:\n      > SELECT first(col) FROM VALUES (10), (5), (20) AS tab(col);\n       10\n      > SELECT first(col) FROM VALUES (NULL), (5), (20) AS tab(col);\n       NULL\n      > SELECT first(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);\n       5\n  \n    Note:\n      The function is non-deterministic because its results depends on the order of the rows\n    which may be non-deterministic after a shuffle.\n\n    Since: 2.0.0\n"
        },
        {
            "name": "first_value",
            "description": "Function: first_value\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.First\nUsage: \n    first_value(expr[, isIgnoreNull]) - Returns the first value of `expr` for a group of rows.\n      If `isIgnoreNull` is true, returns only non-null values.\nExtended Usage:\n    Examples:\n      > SELECT first_value(col) FROM VALUES (10), (5), (20) AS tab(col);\n       10\n      > SELECT first_value(col) FROM VALUES (NULL), (5), (20) AS tab(col);\n       NULL\n      > SELECT first_value(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);\n       5\n  \n    Note:\n      The function is non-deterministic because its results depends on the order of the rows\n    which may be non-deterministic after a shuffle.\n\n    Since: 2.0.0\n"
        },
        {
            "name": "flatten",
            "description": "Function: flatten\nClass: org.apache.spark.sql.catalyst.expressions.Flatten\nUsage: flatten(arrayOfArrays) - Transforms an array of arrays into a single array.\nExtended Usage:\n    Examples:\n      > SELECT flatten(array(array(1, 2), array(3, 4)));\n       [1,2,3,4]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "float",
            "description": "Function: float\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: float(expr) - Casts the value `expr` to the target data type `float`.\nExtended Usage:\n    No example/argument for float.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "floor",
            "description": "Function: floor\nClass: org.apache.spark.sql.catalyst.expressions.Floor\nUsage: floor(expr) - Returns the largest integer not greater than `expr`.\nExtended Usage:\n    Examples:\n      > SELECT floor(-0.1);\n       -1\n      > SELECT floor(5);\n       5\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "forall",
            "description": "Function: forall\nClass: org.apache.spark.sql.catalyst.expressions.ArrayForAll\nUsage: forall(expr, pred) - Tests whether a predicate holds for all elements in the array.\nExtended Usage:\n    Examples:\n      > SELECT forall(array(1, 2, 3), x -> x % 2 == 0);\n       false\n      > SELECT forall(array(2, 4, 8), x -> x % 2 == 0);\n       true\n      > SELECT forall(array(1, null, 3), x -> x % 2 == 0);\n       false\n      > SELECT forall(array(2, null, 8), x -> x % 2 == 0);\n       NULL\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "format_number",
            "description": "Function: format_number\nClass: org.apache.spark.sql.catalyst.expressions.FormatNumber\nUsage: \n    format_number(expr1, expr2) - Formats the number `expr1` like '#,###,###.##', rounded to `expr2`\n      decimal places. If `expr2` is 0, the result has no decimal point or fractional part.\n      `expr2` also accept a user specified format.\n      This is supposed to function like MySQL's FORMAT.\n  \nExtended Usage:\n    Examples:\n      > SELECT format_number(12332.123456, 4);\n       12,332.1235\n      > SELECT format_number(12332.123456, '##################.###');\n       12332.123\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "format_string",
            "description": "Function: format_string\nClass: org.apache.spark.sql.catalyst.expressions.FormatString\nUsage: format_string(strfmt, obj, ...) - Returns a formatted string from printf-style format strings.\nExtended Usage:\n    Examples:\n      > SELECT format_string(\"Hello World %d %s\", 100, \"days\");\n       Hello World 100 days\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "from_csv",
            "description": "Function: from_csv\nClass: org.apache.spark.sql.catalyst.expressions.CsvToStructs\nUsage: from_csv(csvStr, schema[, options]) - Returns a struct value with the given `csvStr` and `schema`.\nExtended Usage:\n    Examples:\n      > SELECT from_csv('1, 0.8', 'a INT, b DOUBLE');\n       {\"a\":1,\"b\":0.8}\n      > SELECT from_csv('26/08/2015', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));\n       {\"time\":2015-08-26 00:00:00}\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "from_json",
            "description": "Function: from_json\nClass: org.apache.spark.sql.catalyst.expressions.JsonToStructs\nUsage: from_json(jsonStr, schema[, options]) - Returns a struct value with the given `jsonStr` and `schema`.\nExtended Usage:\n    Examples:\n      > SELECT from_json('{\"a\":1, \"b\":0.8}', 'a INT, b DOUBLE');\n       {\"a\":1,\"b\":0.8}\n      > SELECT from_json('{\"time\":\"26/08/2015\"}', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));\n       {\"time\":2015-08-26 00:00:00}\n  \n    Since: 2.2.0\n"
        },
        {
            "name": "from_unixtime",
            "description": "Function: from_unixtime\nClass: org.apache.spark.sql.catalyst.expressions.FromUnixTime\nUsage: from_unixtime(unix_time[, fmt]) - Returns `unix_time` in the specified `fmt`.\nExtended Usage:\n    Arguments:\n      * unix_time - UNIX Timestamp to be converted to the provided format.\n      * fmt - Date/time format pattern to follow. See <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\">Datetime Patterns</a>\n              for valid date and time format patterns. The 'yyyy-MM-dd HH:mm:ss' pattern is used if omitted.\n  \n    Examples:\n      > SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss');\n       1969-12-31 16:00:00\n\n      > SELECT from_unixtime(0);\n       1969-12-31 16:00:00\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "from_utc_timestamp",
            "description": "Function: from_utc_timestamp\nClass: org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp\nUsage: from_utc_timestamp(timestamp, timezone) - Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders that time as a timestamp in the given time zone. For example, 'GMT+1' would yield '2017-07-14 03:40:00.0'.\nExtended Usage:\n    Examples:\n      > SELECT from_utc_timestamp('2016-08-31', 'Asia/Seoul');\n       2016-08-31 09:00:00\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "get_json_object",
            "description": "Function: get_json_object\nClass: org.apache.spark.sql.catalyst.expressions.GetJsonObject\nUsage: get_json_object(json_txt, path) - Extracts a json object from `path`.\nExtended Usage:\n    Examples:\n      > SELECT get_json_object('{\"a\":\"b\"}', '$.a');\n       b\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "greatest",
            "description": "Function: greatest\nClass: org.apache.spark.sql.catalyst.expressions.Greatest\nUsage: greatest(expr, ...) - Returns the greatest value of all parameters, skipping null values.\nExtended Usage:\n    Examples:\n      > SELECT greatest(10, 9, 2, 4, 3);\n       10\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "grouping",
            "description": "Function: grouping\nClass: org.apache.spark.sql.catalyst.expressions.Grouping\nUsage: \n    grouping(col) - indicates whether a specified column in a GROUP BY is aggregated or\n      not, returns 1 for aggregated or 0 for not aggregated in the result set.\",\n  \nExtended Usage:\n    Examples:\n      > SELECT name, grouping(name), sum(age) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY cube(name);\n        Alice\t0\t2\n        Bob\t0\t5\n        NULL\t1\t7\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "grouping_id",
            "description": "Function: grouping_id\nClass: org.apache.spark.sql.catalyst.expressions.GroupingID\nUsage: \n    grouping_id([col1[, col2 ..]]) - returns the level of grouping, equals to\n      `(grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)`\n  \nExtended Usage:\n    Examples:\n      > SELECT name, grouping_id(), sum(age), avg(height) FROM VALUES (2, 'Alice', 165), (5, 'Bob', 180) people(age, name, height) GROUP BY cube(name, height);\n        Alice\t0\t2\t165.0\n        Alice\t1\t2\t165.0\n        NULL\t3\t7\t172.5\n        Bob\t0\t5\t180.0\n        Bob\t1\t5\t180.0\n        NULL\t2\t2\t165.0\n        NULL\t2\t5\t180.0\n  \n    Note:\n      Input columns should match with grouping columns exactly, or empty (means all the grouping\n    columns).\n\n    Since: 2.0.0\n"
        },
        {
            "name": "hash",
            "description": "Function: hash\nClass: org.apache.spark.sql.catalyst.expressions.Murmur3Hash\nUsage: hash(expr1, expr2, ...) - Returns a hash value of the arguments.\nExtended Usage:\n    Examples:\n      > SELECT hash('Spark', array(123), 2);\n       -1321691492\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "hex",
            "description": "Function: hex\nClass: org.apache.spark.sql.catalyst.expressions.Hex\nUsage: hex(expr) - Converts `expr` to hexadecimal.\nExtended Usage:\n    Examples:\n      > SELECT hex(17);\n       11\n      > SELECT hex('Spark SQL');\n       537061726B2053514C\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "hour",
            "description": "Function: hour\nClass: org.apache.spark.sql.catalyst.expressions.Hour\nUsage: hour(timestamp) - Returns the hour component of the string/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT hour('2009-07-30 12:58:59');\n       12\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "hypot",
            "description": "Function: hypot\nClass: org.apache.spark.sql.catalyst.expressions.Hypot\nUsage: hypot(expr1, expr2) - Returns sqrt(`expr1`**2 + `expr2`**2).\nExtended Usage:\n    Examples:\n      > SELECT hypot(3, 4);\n       5.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "if",
            "description": "Function: if\nClass: org.apache.spark.sql.catalyst.expressions.If\nUsage: if(expr1, expr2, expr3) - If `expr1` evaluates to true, then returns `expr2`; otherwise returns `expr3`.\nExtended Usage:\n    Examples:\n      > SELECT if(1 < 2, 'a', 'b');\n       a\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "ifnull",
            "description": "Function: ifnull\nClass: org.apache.spark.sql.catalyst.expressions.IfNull\nUsage: ifnull(expr1, expr2) - Returns `expr2` if `expr1` is null, or `expr1` otherwise.\nExtended Usage:\n    Examples:\n      > SELECT ifnull(NULL, array('2'));\n       [\"2\"]\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "in",
            "description": "Function: in\nClass: org.apache.spark.sql.catalyst.expressions.In\nUsage: expr1 in(expr2, expr3, ...) - Returns true if `expr` equals to any valN.\nExtended Usage:\n    Arguments:\n      * expr1, expr2, expr3, ... - the arguments must be same type.\n  \n    Examples:\n      > SELECT 1 in(1, 2, 3);\n       true\n      > SELECT 1 in(2, 3, 4);\n       false\n      > SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 1), named_struct('a', 1, 'b', 3));\n       false\n      > SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 2), named_struct('a', 1, 'b', 3));\n       true\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "initcap",
            "description": "Function: initcap\nClass: org.apache.spark.sql.catalyst.expressions.InitCap\nUsage: \n    initcap(str) - Returns `str` with the first letter of each word in uppercase.\n      All other letters are in lowercase. Words are delimited by white space.\n  \nExtended Usage:\n    Examples:\n      > SELECT initcap('sPark sql');\n       Spark Sql\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "inline",
            "description": "Function: inline\nClass: org.apache.spark.sql.catalyst.expressions.Inline\nUsage: inline(expr) - Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.\nExtended Usage:\n    Examples:\n      > SELECT inline(array(struct(1, 'a'), struct(2, 'b')));\n       1\ta\n       2\tb\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "inline_outer",
            "description": "Function: inline_outer\nClass: org.apache.spark.sql.catalyst.expressions.Inline\nUsage: inline_outer(expr) - Explodes an array of structs into a table. Uses column names col1, col2, etc. by default unless specified otherwise.\nExtended Usage:\n    Examples:\n      > SELECT inline_outer(array(struct(1, 'a'), struct(2, 'b')));\n       1\ta\n       2\tb\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "input_file_block_length",
            "description": "Function: input_file_block_length\nClass: org.apache.spark.sql.catalyst.expressions.InputFileBlockLength\nUsage: input_file_block_length() - Returns the length of the block being read, or -1 if not available.\nExtended Usage:\n    Examples:\n      > SELECT input_file_block_length();\n       -1\n  \n    Since: 2.2.0\n"
        },
        {
            "name": "input_file_block_start",
            "description": "Function: input_file_block_start\nClass: org.apache.spark.sql.catalyst.expressions.InputFileBlockStart\nUsage: input_file_block_start() - Returns the start offset of the block being read, or -1 if not available.\nExtended Usage:\n    Examples:\n      > SELECT input_file_block_start();\n       -1\n  \n    Since: 2.2.0\n"
        },
        {
            "name": "input_file_name",
            "description": "Function: input_file_name\nClass: org.apache.spark.sql.catalyst.expressions.InputFileName\nUsage: input_file_name() - Returns the name of the file being read, or empty string if not available.\nExtended Usage:\n    Examples:\n      > SELECT input_file_name();\n\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "instr",
            "description": "Function: instr\nClass: org.apache.spark.sql.catalyst.expressions.StringInstr\nUsage: instr(str, substr) - Returns the (1-based) index of the first occurrence of `substr` in `str`.\nExtended Usage:\n    Examples:\n      > SELECT instr('SparkSQL', 'SQL');\n       6\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "int",
            "description": "Function: int\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: int(expr) - Casts the value `expr` to the target data type `int`.\nExtended Usage:\n    No example/argument for int.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "isnan",
            "description": "Function: isnan\nClass: org.apache.spark.sql.catalyst.expressions.IsNaN\nUsage: isnan(expr) - Returns true if `expr` is NaN, or false otherwise.\nExtended Usage:\n    Examples:\n      > SELECT isnan(cast('NaN' as double));\n       true\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "isnotnull",
            "description": "Function: isnotnull\nClass: org.apache.spark.sql.catalyst.expressions.IsNotNull\nUsage: isnotnull(expr) - Returns true if `expr` is not null, or false otherwise.\nExtended Usage:\n    Examples:\n      > SELECT isnotnull(1);\n       true\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "isnull",
            "description": "Function: isnull\nClass: org.apache.spark.sql.catalyst.expressions.IsNull\nUsage: isnull(expr) - Returns true if `expr` is null, or false otherwise.\nExtended Usage:\n    Examples:\n      > SELECT isnull(1);\n       false\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "java_method",
            "description": "Function: java_method\nClass: org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection\nUsage: java_method(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.\nExtended Usage:\n    Examples:\n      > SELECT java_method('java.util.UUID', 'randomUUID');\n       c33fb387-8500-4bfa-81d2-6e0e3e930df2\n      > SELECT java_method('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');\n       a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "json_array_length",
            "description": "Function: json_array_length\nClass: org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray\nUsage: json_array_length(jsonArray) - Returns the number of elements in the outmost JSON array.\nExtended Usage:\n    Arguments:\n      * jsonArray - A JSON array. `NULL` is returned in case of any other valid JSON string,\n          `NULL` or an invalid JSON.\n  \n    Examples:\n      > SELECT json_array_length('[1,2,3,4]');\n        4\n      > SELECT json_array_length('[1,2,3,{\"f1\":1,\"f2\":[5,6]},4]');\n        5\n      > SELECT json_array_length('[1,2');\n        NULL\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "json_object_keys",
            "description": "Function: json_object_keys\nClass: org.apache.spark.sql.catalyst.expressions.JsonObjectKeys\nUsage: json_object_keys(json_object) - Returns all the keys of the outmost JSON object as an array.\nExtended Usage:\n    Arguments:\n      * json_object - A JSON object. If a valid JSON object is given, all the keys of the outmost\n          object will be returned as an array. If it is any other valid JSON string, an invalid JSON\n          string or an empty string, the function returns null.\n  \n    Examples:\n      > SELECT json_object_keys('{}');\n        []\n      > SELECT json_object_keys('{\"key\": \"value\"}');\n        [\"key\"]\n      > SELECT json_object_keys('{\"f1\":\"abc\",\"f2\":{\"f3\":\"a\", \"f4\":\"b\"}}');\n        [\"f1\",\"f2\"]\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "json_tuple",
            "description": "Function: json_tuple\nClass: org.apache.spark.sql.catalyst.expressions.JsonTuple\nUsage: json_tuple(jsonStr, p1, p2, ..., pn) - Returns a tuple like the function get_json_object, but it takes multiple names. All the input parameters and output column types are string.\nExtended Usage:\n    Examples:\n      > SELECT json_tuple('{\"a\":1, \"b\":2}', 'a', 'b');\n       1\t2\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "kurtosis",
            "description": "Function: kurtosis\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis\nUsage: kurtosis(expr) - Returns the kurtosis value calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT kurtosis(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);\n       -0.7014368047529627\n      > SELECT kurtosis(col) FROM VALUES (1), (10), (100), (10), (1) as tab(col);\n       0.19432323191699075\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "lag",
            "description": "Function: lag\nClass: org.apache.spark.sql.catalyst.expressions.Lag\nUsage: \n    lag(input[, offset[, default]]) - Returns the value of `input` at the `offset`th row\n      before the current row in the window. The default value of `offset` is 1 and the default\n      value of `default` is null. If the value of `input` at the `offset`th row is null,\n      null is returned. If there is no such offset row (e.g., when the offset is 1, the first\n      row of the window does not have any previous row), `default` is returned.\n  \nExtended Usage:\n    Arguments:\n      * input - a string expression to evaluate `offset` rows before the current row.\n      * offset - an int expression which is rows to jump back in the partition.\n      * default - a string expression which is to use when the offset row does not exist.\n  \n    Examples:\n      > SELECT a, b, lag(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\tNULL\n       A1\t1\t1\n       A1\t2\t1\n       A2\t3\tNULL\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "last",
            "description": "Function: last\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Last\nUsage: \n    last(expr[, isIgnoreNull]) - Returns the last value of `expr` for a group of rows.\n      If `isIgnoreNull` is true, returns only non-null values\nExtended Usage:\n    Examples:\n      > SELECT last(col) FROM VALUES (10), (5), (20) AS tab(col);\n       20\n      > SELECT last(col) FROM VALUES (10), (5), (NULL) AS tab(col);\n       NULL\n      > SELECT last(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);\n       5\n  \n    Note:\n      The function is non-deterministic because its results depends on the order of the rows\n    which may be non-deterministic after a shuffle.\n\n    Since: 2.0.0\n"
        },
        {
            "name": "last_day",
            "description": "Function: last_day\nClass: org.apache.spark.sql.catalyst.expressions.LastDay\nUsage: last_day(date) - Returns the last day of the month which the date belongs to.\nExtended Usage:\n    Examples:\n      > SELECT last_day('2009-01-12');\n       2009-01-31\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "last_value",
            "description": "Function: last_value\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Last\nUsage: \n    last_value(expr[, isIgnoreNull]) - Returns the last value of `expr` for a group of rows.\n      If `isIgnoreNull` is true, returns only non-null values\nExtended Usage:\n    Examples:\n      > SELECT last_value(col) FROM VALUES (10), (5), (20) AS tab(col);\n       20\n      > SELECT last_value(col) FROM VALUES (10), (5), (NULL) AS tab(col);\n       NULL\n      > SELECT last_value(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);\n       5\n  \n    Note:\n      The function is non-deterministic because its results depends on the order of the rows\n    which may be non-deterministic after a shuffle.\n\n    Since: 2.0.0\n"
        },
        {
            "name": "lcase",
            "description": "Function: lcase\nClass: org.apache.spark.sql.catalyst.expressions.Lower\nUsage: lcase(str) - Returns `str` with all characters changed to lowercase.\nExtended Usage:\n    Examples:\n      > SELECT lcase('SparkSql');\n       sparksql\n  \n    Since: 1.0.1\n"
        },
        {
            "name": "lead",
            "description": "Function: lead\nClass: org.apache.spark.sql.catalyst.expressions.Lead\nUsage: \n    lead(input[, offset[, default]]) - Returns the value of `input` at the `offset`th row\n      after the current row in the window. The default value of `offset` is 1 and the default\n      value of `default` is null. If the value of `input` at the `offset`th row is null,\n      null is returned. If there is no such an offset row (e.g., when the offset is 1, the last\n      row of the window does not have any subsequent row), `default` is returned.\n  \nExtended Usage:\n    Arguments:\n      * input - a string expression to evaluate `offset` rows after the current row.\n      * offset - an int expression which is rows to jump ahead in the partition.\n      * default - a string expression which is to use when the offset is larger than the window.\n          The default value is null.\n  \n    Examples:\n      > SELECT a, b, lead(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t1\n       A1\t1\t2\n       A1\t2\tNULL\n       A2\t3\tNULL\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "least",
            "description": "Function: least\nClass: org.apache.spark.sql.catalyst.expressions.Least\nUsage: least(expr, ...) - Returns the least value of all parameters, skipping null values.\nExtended Usage:\n    Examples:\n      > SELECT least(10, 9, 2, 4, 3);\n       2\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "left",
            "description": "Function: left\nClass: org.apache.spark.sql.catalyst.expressions.Left\nUsage: left(str, len) - Returns the leftmost `len`(`len` can be string type) characters from the string `str`,if `len` is less or equal than 0 the result is an empty string.\nExtended Usage:\n    Examples:\n      > SELECT left('Spark SQL', 3);\n       Spa\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "length",
            "description": "Function: length\nClass: org.apache.spark.sql.catalyst.expressions.Length\nUsage: length(expr) - Returns the character length of string data or number of bytes of binary data. The length of string data includes the trailing spaces. The length of binary data includes binary zeros.\nExtended Usage:\n    Examples:\n      > SELECT length('Spark SQL ');\n       10\n      > SELECT CHAR_LENGTH('Spark SQL ');\n       10\n      > SELECT CHARACTER_LENGTH('Spark SQL ');\n       10\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "levenshtein",
            "description": "Function: levenshtein\nClass: org.apache.spark.sql.catalyst.expressions.Levenshtein\nUsage: levenshtein(str1, str2) - Returns the Levenshtein distance between the two given strings.\nExtended Usage:\n    Examples:\n      > SELECT levenshtein('kitten', 'sitting');\n       3\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "like",
            "description": "Function: like\nClass: org.apache.spark.sql.catalyst.expressions.Like\nUsage: str like pattern[ ESCAPE escape] - Returns true if str matches `pattern` with `escape`, null if any arguments are null, false otherwise.\nExtended Usage:\n    Arguments:\n      * str - a string expression\n      * pattern - a string expression. The pattern is a string which is matched literally, with\n          exception to the following special symbols:\n\n          _ matches any one character in the input (similar to . in posix regular expressions)\n\n          % matches zero or more characters in the input (similar to .* in posix regular\n          expressions)\n\n          Since Spark 2.0, string literals are unescaped in our SQL parser. For example, in order\n          to match \"\\abc\", the pattern should be \"\\\\abc\".\n\n          When SQL config 'spark.sql.parser.escapedStringLiterals' is enabled, it fallbacks\n          to Spark 1.6 behavior regarding string literal parsing. For example, if the config is\n          enabled, the pattern to match \"\\abc\" should be \"\\abc\".\n      * escape - an character added since Spark 3.0. The default escape character is the '\\'.\n          If an escape character precedes a special symbol or another escape character, the\n          following character is matched literally. It is invalid to escape any other character.\n  \n    Examples:\n      > SELECT like('Spark', '_park');\n      true\n      > SET spark.sql.parser.escapedStringLiterals=true;\n      spark.sql.parser.escapedStringLiterals\ttrue\n      > SELECT '%SystemDrive%\\Users\\John' like '\\%SystemDrive\\%\\\\Users%';\n      true\n      > SET spark.sql.parser.escapedStringLiterals=false;\n      spark.sql.parser.escapedStringLiterals\tfalse\n      > SELECT '%SystemDrive%\\\\Users\\\\John' like '\\%SystemDrive\\%\\\\\\\\Users%';\n      true\n      > SELECT '%SystemDrive%/Users/John' like '/%SystemDrive/%//Users%' ESCAPE '/';\n      true\n  \n    Note:\n      Use RLIKE to match with standard regular expressions.\n\n    Since: 1.0.0\n"
        },
        {
            "name": "ln",
            "description": "Function: ln\nClass: org.apache.spark.sql.catalyst.expressions.Log\nUsage: ln(expr) - Returns the natural logarithm (base e) of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT ln(1);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "locate",
            "description": "Function: locate\nClass: org.apache.spark.sql.catalyst.expressions.StringLocate\nUsage: \n    locate(substr, str[, pos]) - Returns the position of the first occurrence of `substr` in `str` after position `pos`.\n      The given `pos` and return value are 1-based.\n  \nExtended Usage:\n    Examples:\n      > SELECT locate('bar', 'foobarbar');\n       4\n      > SELECT locate('bar', 'foobarbar', 5);\n       7\n      > SELECT POSITION('bar' IN 'foobarbar');\n       4\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "log",
            "description": "Function: log\nClass: org.apache.spark.sql.catalyst.expressions.Logarithm\nUsage: log(base, expr) - Returns the logarithm of `expr` with `base`.\nExtended Usage:\n    Examples:\n      > SELECT log(10, 100);\n       2.0\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "log10",
            "description": "Function: log10\nClass: org.apache.spark.sql.catalyst.expressions.Log10\nUsage: log10(expr) - Returns the logarithm of `expr` with base 10.\nExtended Usage:\n    Examples:\n      > SELECT log10(10);\n       1.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "log1p",
            "description": "Function: log1p\nClass: org.apache.spark.sql.catalyst.expressions.Log1p\nUsage: log1p(expr) - Returns log(1 + `expr`).\nExtended Usage:\n    Examples:\n      > SELECT log1p(0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "log2",
            "description": "Function: log2\nClass: org.apache.spark.sql.catalyst.expressions.Log2\nUsage: log2(expr) - Returns the logarithm of `expr` with base 2.\nExtended Usage:\n    Examples:\n      > SELECT log2(2);\n       1.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "lower",
            "description": "Function: lower\nClass: org.apache.spark.sql.catalyst.expressions.Lower\nUsage: lower(str) - Returns `str` with all characters changed to lowercase.\nExtended Usage:\n    Examples:\n      > SELECT lower('SparkSql');\n       sparksql\n  \n    Since: 1.0.1\n"
        },
        {
            "name": "lpad",
            "description": "Function: lpad\nClass: org.apache.spark.sql.catalyst.expressions.StringLPad\nUsage: \n    lpad(str, len[, pad]) - Returns `str`, left-padded with `pad` to a length of `len`.\n      If `str` is longer than `len`, the return value is shortened to `len` characters.\n      If `pad` is not specified, `str` will be padded to the left with space characters.\n  \nExtended Usage:\n    Examples:\n      > SELECT lpad('hi', 5, '??');\n       ???hi\n      > SELECT lpad('hi', 1, '??');\n       h\n      > SELECT lpad('hi', 5);\n          hi\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "ltrim",
            "description": "Function: ltrim\nClass: org.apache.spark.sql.catalyst.expressions.StringTrimLeft\nUsage: \n    ltrim(str) - Removes the leading space characters from `str`.\n  \nExtended Usage:\n    Arguments:\n      * str - a string expression\n      * trimStr - the trim string characters to trim, the default value is a single space\n  \n    Examples:\n      > SELECT ltrim('    SparkSQL   ');\n       SparkSQL\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "make_date",
            "description": "Function: make_date\nClass: org.apache.spark.sql.catalyst.expressions.MakeDate\nUsage: make_date(year, month, day) - Create date from year, month and day fields.\nExtended Usage:\n    Arguments:\n      * year - the year to represent, from 1 to 9999\n      * month - the month-of-year to represent, from 1 (January) to 12 (December)\n      * day - the day-of-month to represent, from 1 to 31\n  \n    Examples:\n      > SELECT make_date(2013, 7, 15);\n       2013-07-15\n      > SELECT make_date(2019, 13, 1);\n       NULL\n      > SELECT make_date(2019, 7, NULL);\n       NULL\n      > SELECT make_date(2019, 2, 30);\n       NULL\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "make_interval",
            "description": "Function: make_interval\nClass: org.apache.spark.sql.catalyst.expressions.MakeInterval\nUsage: make_interval(years, months, weeks, days, hours, mins, secs) - Make interval from years, months, weeks, days, hours, mins and secs.\nExtended Usage:\n    Arguments:\n      * years - the number of years, positive or negative\n      * months - the number of months, positive or negative\n      * weeks - the number of weeks, positive or negative\n      * days - the number of days, positive or negative\n      * hours - the number of hours, positive or negative\n      * mins - the number of minutes, positive or negative\n      * secs - the number of seconds with the fractional part in microsecond precision.\n  \n    Examples:\n      > SELECT make_interval(100, 11, 1, 1, 12, 30, 01.001001);\n       100 years 11 months 8 days 12 hours 30 minutes 1.001001 seconds\n      > SELECT make_interval(100, null, 3);\n       NULL\n      > SELECT make_interval(0, 1, 0, 1, 0, 0, 100.000001);\n       1 months 1 days 1 minutes 40.000001 seconds\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "make_timestamp",
            "description": "Function: make_timestamp\nClass: org.apache.spark.sql.catalyst.expressions.MakeTimestamp\nUsage: make_timestamp(year, month, day, hour, min, sec[, timezone]) - Create timestamp from year, month, day, hour, min, sec and timezone fields.\nExtended Usage:\n    Arguments:\n      * year - the year to represent, from 1 to 9999\n      * month - the month-of-year to represent, from 1 (January) to 12 (December)\n      * day - the day-of-month to represent, from 1 to 31\n      * hour - the hour-of-day to represent, from 0 to 23\n      * min - the minute-of-hour to represent, from 0 to 59\n      * sec - the second-of-minute and its micro-fraction to represent, from\n              0 to 60. If the sec argument equals to 60, the seconds field is set\n              to 0 and 1 minute is added to the final timestamp.\n      * timezone - the time zone identifier. For example, CET, UTC and etc.\n  \n    Examples:\n      > SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887);\n       2014-12-28 06:30:45.887\n      > SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887, 'CET');\n       2014-12-27 21:30:45.887\n      > SELECT make_timestamp(2019, 6, 30, 23, 59, 60);\n       2019-07-01 00:00:00\n      > SELECT make_timestamp(2019, 13, 1, 10, 11, 12, 'PST');\n       NULL\n      > SELECT make_timestamp(null, 7, 22, 15, 30, 0);\n       NULL\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "map",
            "description": "Function: map\nClass: org.apache.spark.sql.catalyst.expressions.CreateMap\nUsage: map(key0, value0, key1, value1, ...) - Creates a map with the given key/value pairs.\nExtended Usage:\n    Examples:\n      > SELECT map(1.0, '2', 3.0, '4');\n       {1.0:\"2\",3.0:\"4\"}\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "map_concat",
            "description": "Function: map_concat\nClass: org.apache.spark.sql.catalyst.expressions.MapConcat\nUsage: map_concat(map, ...) - Returns the union of all the given maps\nExtended Usage:\n    Examples:\n      > SELECT map_concat(map(1, 'a', 2, 'b'), map(3, 'c'));\n       {1:\"a\",2:\"b\",3:\"c\"}\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "map_entries",
            "description": "Function: map_entries\nClass: org.apache.spark.sql.catalyst.expressions.MapEntries\nUsage: map_entries(map) - Returns an unordered array of all entries in the given map.\nExtended Usage:\n    Examples:\n      > SELECT map_entries(map(1, 'a', 2, 'b'));\n       [{\"key\":1,\"value\":\"a\"},{\"key\":2,\"value\":\"b\"}]\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "map_filter",
            "description": "Function: map_filter\nClass: org.apache.spark.sql.catalyst.expressions.MapFilter\nUsage: map_filter(expr, func) - Filters entries in a map using the function.\nExtended Usage:\n    Examples:\n      > SELECT map_filter(map(1, 0, 2, 2, 3, -1), (k, v) -> k > v);\n       {1:0,3:-1}\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "map_from_arrays",
            "description": "Function: map_from_arrays\nClass: org.apache.spark.sql.catalyst.expressions.MapFromArrays\nUsage: \n    map_from_arrays(keys, values) - Creates a map with a pair of the given key/value arrays. All elements\n      in keys should not be null\nExtended Usage:\n    Examples:\n      > SELECT map_from_arrays(array(1.0, 3.0), array('2', '4'));\n       {1.0:\"2\",3.0:\"4\"}\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "map_from_entries",
            "description": "Function: map_from_entries\nClass: org.apache.spark.sql.catalyst.expressions.MapFromEntries\nUsage: map_from_entries(arrayOfEntries) - Returns a map created from the given array of entries.\nExtended Usage:\n    Examples:\n      > SELECT map_from_entries(array(struct(1, 'a'), struct(2, 'b')));\n       {1:\"a\",2:\"b\"}\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "map_keys",
            "description": "Function: map_keys\nClass: org.apache.spark.sql.catalyst.expressions.MapKeys\nUsage: map_keys(map) - Returns an unordered array containing the keys of the map.\nExtended Usage:\n    Examples:\n      > SELECT map_keys(map(1, 'a', 2, 'b'));\n       [1,2]\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "map_values",
            "description": "Function: map_values\nClass: org.apache.spark.sql.catalyst.expressions.MapValues\nUsage: map_values(map) - Returns an unordered array containing the values of the map.\nExtended Usage:\n    Examples:\n      > SELECT map_values(map(1, 'a', 2, 'b'));\n       [\"a\",\"b\"]\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "map_zip_with",
            "description": "Function: map_zip_with\nClass: org.apache.spark.sql.catalyst.expressions.MapZipWith\nUsage: \n      map_zip_with(map1, map2, function) - Merges two given maps into a single map by applying\n      function to the pair of values with the same key. For keys only presented in one map,\n      NULL will be passed as the value for the missing key. If an input map contains duplicated\n      keys, only the first entry of the duplicated key is passed into the lambda function.\n    \nExtended Usage:\n    Examples:\n      > SELECT map_zip_with(map(1, 'a', 2, 'b'), map(1, 'x', 2, 'y'), (k, v1, v2) -> concat(v1, v2));\n       {1:\"ax\",2:\"by\"}\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "max",
            "description": "Function: max\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Max\nUsage: max(expr) - Returns the maximum value of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT max(col) FROM VALUES (10), (50), (20) AS tab(col);\n       50\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "max_by",
            "description": "Function: max_by\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy\nUsage: max_by(x, y) - Returns the value of `x` associated with the maximum value of `y`.\nExtended Usage:\n    Examples:\n      > SELECT max_by(x, y) FROM VALUES (('a', 10)), (('b', 50)), (('c', 20)) AS tab(x, y);\n       b\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "md5",
            "description": "Function: md5\nClass: org.apache.spark.sql.catalyst.expressions.Md5\nUsage: md5(expr) - Returns an MD5 128-bit checksum as a hex string of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT md5('Spark');\n       8cde774d6f7333752ed72cacddb05126\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "mean",
            "description": "Function: mean\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Average\nUsage: mean(expr) - Returns the mean calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT mean(col) FROM VALUES (1), (2), (3) AS tab(col);\n       2.0\n      > SELECT mean(col) FROM VALUES (1), (2), (NULL) AS tab(col);\n       1.5\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "min",
            "description": "Function: min\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Min\nUsage: min(expr) - Returns the minimum value of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT min(col) FROM VALUES (10), (-1), (20) AS tab(col);\n       -1\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "min_by",
            "description": "Function: min_by\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.MinBy\nUsage: min_by(x, y) - Returns the value of `x` associated with the minimum value of `y`.\nExtended Usage:\n    Examples:\n      > SELECT min_by(x, y) FROM VALUES (('a', 10)), (('b', 50)), (('c', 20)) AS tab(x, y);\n       a\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "minute",
            "description": "Function: minute\nClass: org.apache.spark.sql.catalyst.expressions.Minute\nUsage: minute(timestamp) - Returns the minute component of the string/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT minute('2009-07-30 12:58:59');\n       58\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "mod",
            "description": "Function: mod\nClass: org.apache.spark.sql.catalyst.expressions.Remainder\nUsage: expr1 mod expr2 - Returns the remainder after `expr1`/`expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 2 % 1.8;\n       0.2\n      > SELECT MOD(2, 1.8);\n       0.2\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "monotonically_increasing_id",
            "description": "Function: monotonically_increasing_id\nClass: org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID\nUsage: \n    monotonically_increasing_id() - Returns monotonically increasing 64-bit integers. The generated ID is guaranteed\n      to be monotonically increasing and unique, but not consecutive. The current implementation\n      puts the partition ID in the upper 31 bits, and the lower 33 bits represent the record number\n      within each partition. The assumption is that the data frame has less than 1 billion\n      partitions, and each partition has less than 8 billion records.\n      The function is non-deterministic because its result depends on partition IDs.\n  \nExtended Usage:\n    Examples:\n      > SELECT monotonically_increasing_id();\n       0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "month",
            "description": "Function: month\nClass: org.apache.spark.sql.catalyst.expressions.Month\nUsage: month(date) - Returns the month component of the date/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT month('2016-07-30');\n       7\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "months_between",
            "description": "Function: months_between\nClass: org.apache.spark.sql.catalyst.expressions.MonthsBetween\nUsage: \n    months_between(timestamp1, timestamp2[, roundOff]) - If `timestamp1` is later than `timestamp2`, then the result\n      is positive. If `timestamp1` and `timestamp2` are on the same day of month, or both\n      are the last day of month, time of day will be ignored. Otherwise, the difference is\n      calculated based on 31 days per month, and rounded to 8 digits unless roundOff=false.\n  \nExtended Usage:\n    Examples:\n      > SELECT months_between('1997-02-28 10:30:00', '1996-10-30');\n       3.94959677\n      > SELECT months_between('1997-02-28 10:30:00', '1996-10-30', false);\n       3.9495967741935485\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "named_struct",
            "description": "Function: named_struct\nClass: org.apache.spark.sql.catalyst.expressions.CreateNamedStruct\nUsage: named_struct(name1, val1, name2, val2, ...) - Creates a struct with the given field names and values.\nExtended Usage:\n    Examples:\n      > SELECT named_struct(\"a\", 1, \"b\", 2, \"c\", 3);\n       {\"a\":1,\"b\":2,\"c\":3}\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "nanvl",
            "description": "Function: nanvl\nClass: org.apache.spark.sql.catalyst.expressions.NaNvl\nUsage: nanvl(expr1, expr2) - Returns `expr1` if it's not NaN, or `expr2` otherwise.\nExtended Usage:\n    Examples:\n      > SELECT nanvl(cast('NaN' as double), 123);\n       123.0\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "negative",
            "description": "Function: negative\nClass: org.apache.spark.sql.catalyst.expressions.UnaryMinus\nUsage: negative(expr) - Returns the negated value of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT negative(1);\n       -1\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "next_day",
            "description": "Function: next_day\nClass: org.apache.spark.sql.catalyst.expressions.NextDay\nUsage: next_day(start_date, day_of_week) - Returns the first date which is later than `start_date` and named as indicated.\nExtended Usage:\n    Examples:\n      > SELECT next_day('2015-01-14', 'TU');\n       2015-01-20\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "not",
            "description": "Function: not\nClass: org.apache.spark.sql.catalyst.expressions.Not\nUsage: not expr - Logical not.\nExtended Usage:\n    Examples:\n      > SELECT not true;\n       false\n      > SELECT not false;\n       true\n      > SELECT not NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "now",
            "description": "Function: now\nClass: org.apache.spark.sql.catalyst.expressions.Now\nUsage: now() - Returns the current timestamp at the start of query evaluation.\nExtended Usage:\n    Examples:\n      > SELECT now();\n       2020-04-25 15:49:11.914\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "nth_value",
            "description": "Function: nth_value\nClass: org.apache.spark.sql.catalyst.expressions.NthValue\nUsage: \n    nth_value(input[, offset]) - Returns the value of `input` at the row that is the `offset`th row\n      from beginning of the window frame. Offset starts at 1. If ignoreNulls=true, we will skip\n      nulls when finding the `offset`th row. Otherwise, every row counts for the `offset`. If\n      there is no such an `offset`th row (e.g., when the offset is 10, size of the window frame\n      is less than 10), null is returned.\n  \nExtended Usage:\n    Arguments:\n      * input - the target column or expression that the function operates on.\n      * offset - a positive int literal to indicate the offset in the window frame. It starts\n          with 1.\n      * ignoreNulls - an optional specification that indicates the NthValue should skip null\n          values in the determination of which row to use.\n  \n    Examples:\n      > SELECT a, b, nth_value(b, 2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t1\n       A1\t1\t1\n       A1\t2\t1\n       A2\t3\tNULL\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "ntile",
            "description": "Function: ntile\nClass: org.apache.spark.sql.catalyst.expressions.NTile\nUsage: \n    ntile(n) - Divides the rows for each window partition into `n` buckets ranging\n      from 1 to at most `n`.\n  \nExtended Usage:\n    Arguments:\n      * buckets - an int expression which is number of buckets to divide the rows in.\n          Default value is 1.\n  \n    Examples:\n      > SELECT a, b, ntile(2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t1\n       A1\t1\t1\n       A1\t2\t2\n       A2\t3\t1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "nullif",
            "description": "Function: nullif\nClass: org.apache.spark.sql.catalyst.expressions.NullIf\nUsage: nullif(expr1, expr2) - Returns null if `expr1` equals to `expr2`, or `expr1` otherwise.\nExtended Usage:\n    Examples:\n      > SELECT nullif(2, 2);\n       NULL\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "nvl",
            "description": "Function: nvl\nClass: org.apache.spark.sql.catalyst.expressions.Nvl\nUsage: nvl(expr1, expr2) - Returns `expr2` if `expr1` is null, or `expr1` otherwise.\nExtended Usage:\n    Examples:\n      > SELECT nvl(NULL, array('2'));\n       [\"2\"]\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "nvl2",
            "description": "Function: nvl2\nClass: org.apache.spark.sql.catalyst.expressions.Nvl2\nUsage: nvl2(expr1, expr2, expr3) - Returns `expr2` if `expr1` is not null, or `expr3` otherwise.\nExtended Usage:\n    Examples:\n      > SELECT nvl2(NULL, 2, 1);\n       1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "octet_length",
            "description": "Function: octet_length\nClass: org.apache.spark.sql.catalyst.expressions.OctetLength\nUsage: octet_length(expr) - Returns the byte length of string data or number of bytes of binary data.\nExtended Usage:\n    Examples:\n      > SELECT octet_length('Spark SQL');\n       9\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "or",
            "description": "Function: or\nClass: org.apache.spark.sql.catalyst.expressions.Or\nUsage: expr1 or expr2 - Logical OR.\nExtended Usage:\n    Examples:\n      > SELECT true or false;\n       true\n      > SELECT false or false;\n       false\n      > SELECT true or NULL;\n       true\n      > SELECT false or NULL;\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "overlay",
            "description": "Function: overlay\nClass: org.apache.spark.sql.catalyst.expressions.Overlay\nUsage: overlay(input, replace, pos[, len]) - Replace `input` with `replace` that starts at `pos` and is of length `len`.\nExtended Usage:\n    Examples:\n      > SELECT overlay('Spark SQL' PLACING '_' FROM 6);\n       Spark_SQL\n      > SELECT overlay('Spark SQL' PLACING 'CORE' FROM 7);\n       Spark CORE\n      > SELECT overlay('Spark SQL' PLACING 'ANSI ' FROM 7 FOR 0);\n       Spark ANSI SQL\n      > SELECT overlay('Spark SQL' PLACING 'tructured' FROM 2 FOR 4);\n       Structured SQL\n      > SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('_', 'utf-8') FROM 6);\n       Spark_SQL\n      > SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('CORE', 'utf-8') FROM 7);\n       Spark CORE\n      > SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('ANSI ', 'utf-8') FROM 7 FOR 0);\n       Spark ANSI SQL\n      > SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('tructured', 'utf-8') FROM 2 FOR 4);\n       Structured SQL\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "parse_url",
            "description": "Function: parse_url\nClass: org.apache.spark.sql.catalyst.expressions.ParseUrl\nUsage: parse_url(url, partToExtract[, key]) - Extracts a part from a URL.\nExtended Usage:\n    Examples:\n      > SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST');\n       spark.apache.org\n      > SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY');\n       query=1\n      > SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query');\n       1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "percent_rank",
            "description": "Function: percent_rank\nClass: org.apache.spark.sql.catalyst.expressions.PercentRank\nUsage: \n    percent_rank() - Computes the percentage ranking of a value in a group of values.\n  \nExtended Usage:\n    Arguments:\n      * children - this is to base the rank on; a change in the value of one the children will\n          trigger a change in rank. This is an internal parameter and will be assigned by the\n          Analyser.\n  \n    Examples:\n      > SELECT a, b, percent_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t0.0\n       A1\t1\t0.0\n       A1\t2\t1.0\n       A2\t3\t0.0\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "percentile",
            "description": "Function: percentile\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Percentile\nUsage: \n      percentile(col, percentage [, frequency]) - Returns the exact percentile value of numeric column\n       `col` at the given percentage. The value of percentage must be between 0.0 and 1.0. The\n       value of frequency should be positive integral\n\n      percentile(col, array(percentage1 [, percentage2]...) [, frequency]) - Returns the exact\n      percentile value array of numeric column `col` at the given percentage(s). Each value\n      of the percentage array must be between 0.0 and 1.0. The value of frequency should be\n      positive integral\n\n      \nExtended Usage:\n    Examples:\n      > SELECT percentile(col, 0.3) FROM VALUES (0), (10) AS tab(col);\n       3.0\n      > SELECT percentile(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col);\n       [2.5,7.5]\n  \n    Since: 2.1.0\n"
        },
        {
            "name": "percentile_approx",
            "description": "Function: percentile_approx\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile\nUsage: \n    percentile_approx(col, percentage [, accuracy]) - Returns the approximate `percentile` of the numeric\n      column `col` which is the smallest value in the ordered `col` values (sorted from least to\n      greatest) such that no more than `percentage` of `col` values is less than the value\n      or equal to that value. The value of percentage must be between 0.0 and 1.0. The `accuracy`\n      parameter (default: 10000) is a positive numeric literal which controls approximation accuracy\n      at the cost of memory. Higher value of `accuracy` yields better accuracy, `1.0/accuracy` is\n      the relative error of the approximation.\n      When `percentage` is an array, each value of the percentage array must be between 0.0 and 1.0.\n      In this case, returns the approximate percentile array of column `col` at the given\n      percentage array.\n  \nExtended Usage:\n    Examples:\n      > SELECT percentile_approx(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);\n       [1,1,0]\n      > SELECT percentile_approx(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);\n       7\n  \n    Since: 2.1.0\n"
        },
        {
            "name": "pi",
            "description": "Function: pi\nClass: org.apache.spark.sql.catalyst.expressions.Pi\nUsage: pi() - Returns pi.\nExtended Usage:\n    Examples:\n      > SELECT pi();\n       3.141592653589793\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "pmod",
            "description": "Function: pmod\nClass: org.apache.spark.sql.catalyst.expressions.Pmod\nUsage: pmod(expr1, expr2) - Returns the positive value of `expr1` mod `expr2`.\nExtended Usage:\n    Examples:\n      > SELECT pmod(10, 3);\n       1\n      > SELECT pmod(-10, 3);\n       2\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "posexplode",
            "description": "Function: posexplode\nClass: org.apache.spark.sql.catalyst.expressions.PosExplode\nUsage: posexplode(expr) - Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Unless specified otherwise, uses the column name `pos` for position, `col` for elements of the array or `key` and `value` for elements of the map.\nExtended Usage:\n    Examples:\n      > SELECT posexplode(array(10,20));\n       0\t10\n       1\t20\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "posexplode_outer",
            "description": "Function: posexplode_outer\nClass: org.apache.spark.sql.catalyst.expressions.PosExplode\nUsage: posexplode_outer(expr) - Separates the elements of array `expr` into multiple rows with positions, or the elements of map `expr` into multiple rows and columns with positions. Unless specified otherwise, uses the column name `pos` for position, `col` for elements of the array or `key` and `value` for elements of the map.\nExtended Usage:\n    Examples:\n      > SELECT posexplode_outer(array(10,20));\n       0\t10\n       1\t20\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "position",
            "description": "Function: position\nClass: org.apache.spark.sql.catalyst.expressions.StringLocate\nUsage: \n    position(substr, str[, pos]) - Returns the position of the first occurrence of `substr` in `str` after position `pos`.\n      The given `pos` and return value are 1-based.\n  \nExtended Usage:\n    Examples:\n      > SELECT position('bar', 'foobarbar');\n       4\n      > SELECT position('bar', 'foobarbar', 5);\n       7\n      > SELECT POSITION('bar' IN 'foobarbar');\n       4\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "positive",
            "description": "Function: positive\nClass: org.apache.spark.sql.catalyst.expressions.UnaryPositive\nUsage: positive(expr) - Returns the value of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT positive(1);\n       1\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "pow",
            "description": "Function: pow\nClass: org.apache.spark.sql.catalyst.expressions.Pow\nUsage: pow(expr1, expr2) - Raises `expr1` to the power of `expr2`.\nExtended Usage:\n    Examples:\n      > SELECT pow(2, 3);\n       8.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "power",
            "description": "Function: power\nClass: org.apache.spark.sql.catalyst.expressions.Pow\nUsage: power(expr1, expr2) - Raises `expr1` to the power of `expr2`.\nExtended Usage:\n    Examples:\n      > SELECT power(2, 3);\n       8.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "printf",
            "description": "Function: printf\nClass: org.apache.spark.sql.catalyst.expressions.FormatString\nUsage: printf(strfmt, obj, ...) - Returns a formatted string from printf-style format strings.\nExtended Usage:\n    Examples:\n      > SELECT printf(\"Hello World %d %s\", 100, \"days\");\n       Hello World 100 days\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "quarter",
            "description": "Function: quarter\nClass: org.apache.spark.sql.catalyst.expressions.Quarter\nUsage: quarter(date) - Returns the quarter of the year for date, in the range 1 to 4.\nExtended Usage:\n    Examples:\n      > SELECT quarter('2016-08-31');\n       3\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "radians",
            "description": "Function: radians\nClass: org.apache.spark.sql.catalyst.expressions.ToRadians\nUsage: radians(expr) - Converts degrees to radians.\nExtended Usage:\n    Arguments:\n      * expr - angle in degrees\n  \n    Examples:\n      > SELECT radians(180);\n       3.141592653589793\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "raise_error",
            "description": "Function: raise_error\nClass: org.apache.spark.sql.catalyst.expressions.RaiseError\nUsage: raise_error(expr) - Throws an exception with `expr`.\nExtended Usage:\n    Examples:\n      > SELECT raise_error('custom error message');\n       java.lang.RuntimeException\n       custom error message\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "rand",
            "description": "Function: rand\nClass: org.apache.spark.sql.catalyst.expressions.Rand\nUsage: rand([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).\nExtended Usage:\n    Examples:\n      > SELECT rand();\n       0.9629742951434543\n      > SELECT rand(0);\n       0.8446490682263027\n      > SELECT rand(null);\n       0.8446490682263027\n  \n    Note:\n      The function is non-deterministic in general case.\n\n    Since: 1.5.0\n"
        },
        {
            "name": "randn",
            "description": "Function: randn\nClass: org.apache.spark.sql.catalyst.expressions.Randn\nUsage: randn([seed]) - Returns a random value with independent and identically distributed (i.i.d.) values drawn from the standard normal distribution.\nExtended Usage:\n    Examples:\n      > SELECT randn();\n       -0.3254147983080288\n      > SELECT randn(0);\n       1.1164209726833079\n      > SELECT randn(null);\n       1.1164209726833079\n  \n    Note:\n      The function is non-deterministic in general case.\n\n    Since: 1.5.0\n"
        },
        {
            "name": "random",
            "description": "Function: random\nClass: org.apache.spark.sql.catalyst.expressions.Rand\nUsage: random([seed]) - Returns a random value with independent and identically distributed (i.i.d.) uniformly distributed values in [0, 1).\nExtended Usage:\n    Examples:\n      > SELECT random();\n       0.9629742951434543\n      > SELECT random(0);\n       0.8446490682263027\n      > SELECT random(null);\n       0.8446490682263027\n  \n    Note:\n      The function is non-deterministic in general case.\n\n    Since: 1.5.0\n"
        },
        {
            "name": "rank",
            "description": "Function: rank\nClass: org.apache.spark.sql.catalyst.expressions.Rank\nUsage: \n    rank() - Computes the rank of a value in a group of values. The result is one plus the number\n      of rows preceding or equal to the current row in the ordering of the partition. The values\n      will produce gaps in the sequence.\n  \nExtended Usage:\n    Arguments:\n      * children - this is to base the rank on; a change in the value of one the children will\n          trigger a change in rank. This is an internal parameter and will be assigned by the\n          Analyser.\n  \n    Examples:\n      > SELECT a, b, rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t1\n       A1\t1\t1\n       A1\t2\t3\n       A2\t3\t1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "reflect",
            "description": "Function: reflect\nClass: org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection\nUsage: reflect(class, method[, arg1[, arg2 ..]]) - Calls a method with reflection.\nExtended Usage:\n    Examples:\n      > SELECT reflect('java.util.UUID', 'randomUUID');\n       c33fb387-8500-4bfa-81d2-6e0e3e930df2\n      > SELECT reflect('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');\n       a5cf6c42-0c85-418f-af6c-3e4e5b1328f2\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "regexp_extract",
            "description": "Function: regexp_extract\nClass: org.apache.spark.sql.catalyst.expressions.RegExpExtract\nUsage: \n    regexp_extract(str, regexp[, idx]) - Extract the first string in the `str` that match the `regexp`\n    expression and corresponding to the regex group index.\n  \nExtended Usage:\n    Arguments:\n      * str - a string expression.\n      * regexp - a string representing a regular expression. The regex string should be a\n          Java regular expression.\n\n          Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL\n          parser. For example, to match \"\\abc\", a regular expression for `regexp` can be\n          \"^\\\\abc$\".\n\n          There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to\n          fallback to the Spark 1.6 behavior regarding string literal parsing. For example,\n          if the config is enabled, the `regexp` that can match \"\\abc\" is \"^\\abc$\".\n      * idx - an integer expression that representing the group index. The regex maybe contains\n          multiple groups. `idx` indicates which regex group to extract. The group index should\n          be non-negative. The minimum value of `idx` is 0, which means matching the entire\n          regular expression. If `idx` is not specified, the default group index value is 1. The\n          `idx` parameter is the Java regex Matcher group() method index.\n  \n    Examples:\n      > SELECT regexp_extract('100-200', '(\\\\d+)-(\\\\d+)', 1);\n       100\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "regexp_extract_all",
            "description": "Function: regexp_extract_all\nClass: org.apache.spark.sql.catalyst.expressions.RegExpExtractAll\nUsage: \n    regexp_extract_all(str, regexp[, idx]) - Extract all strings in the `str` that match the `regexp`\n    expression and corresponding to the regex group index.\n  \nExtended Usage:\n    Arguments:\n      * str - a string expression.\n      * regexp - a string representing a regular expression. The regex string should be a\n          Java regular expression.\n\n          Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL\n          parser. For example, to match \"\\abc\", a regular expression for `regexp` can be\n          \"^\\\\abc$\".\n\n          There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to\n          fallback to the Spark 1.6 behavior regarding string literal parsing. For example,\n          if the config is enabled, the `regexp` that can match \"\\abc\" is \"^\\abc$\".\n      * idx - an integer expression that representing the group index. The regex may contains\n          multiple groups. `idx` indicates which regex group to extract. The group index should\n          be non-negative. The minimum value of `idx` is 0, which means matching the entire\n          regular expression. If `idx` is not specified, the default group index value is 1. The\n          `idx` parameter is the Java regex Matcher group() method index.\n  \n    Examples:\n      > SELECT regexp_extract_all('100-200, 300-400', '(\\\\d+)-(\\\\d+)', 1);\n       [\"100\",\"300\"]\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "regexp_replace",
            "description": "Function: regexp_replace\nClass: org.apache.spark.sql.catalyst.expressions.RegExpReplace\nUsage: regexp_replace(str, regexp, rep[, position]) - Replaces all substrings of `str` that match `regexp` with `rep`.\nExtended Usage:\n    Arguments:\n      * str - a string expression to search for a regular expression pattern match.\n      * regexp - a string representing a regular expression. The regex string should be a\n          Java regular expression.\n\n          Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL\n          parser. For example, to match \"\\abc\", a regular expression for `regexp` can be\n          \"^\\\\abc$\".\n\n          There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to\n          fallback to the Spark 1.6 behavior regarding string literal parsing. For example,\n          if the config is enabled, the `regexp` that can match \"\\abc\" is \"^\\abc$\".\n      * rep - a string expression to replace matched substrings.\n      * position - a positive integer literal that indicates the position within `str` to begin searching.\n          The default is 1. If position is greater than the number of characters in `str`, the result is `str`.\n  \n    Examples:\n      > SELECT regexp_replace('100-200', '(\\\\d+)', 'num');\n       num-num\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "repeat",
            "description": "Function: repeat\nClass: org.apache.spark.sql.catalyst.expressions.StringRepeat\nUsage: repeat(str, n) - Returns the string which repeats the given string value n times.\nExtended Usage:\n    Examples:\n      > SELECT repeat('123', 2);\n       123123\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "replace",
            "description": "Function: replace\nClass: org.apache.spark.sql.catalyst.expressions.StringReplace\nUsage: replace(str, search[, replace]) - Replaces all occurrences of `search` with `replace`.\nExtended Usage:\n    Arguments:\n      * str - a string expression\n      * search - a string expression. If `search` is not found in `str`, `str` is returned unchanged.\n      * replace - a string expression. If `replace` is not specified or is an empty string, nothing replaces\n          the string that is removed from `str`.\n  \n    Examples:\n      > SELECT replace('ABCabc', 'abc', 'DEF');\n       ABCDEF\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "reverse",
            "description": "Function: reverse\nClass: org.apache.spark.sql.catalyst.expressions.Reverse\nUsage: reverse(array) - Returns a reversed string or an array with reverse order of elements.\nExtended Usage:\n    Examples:\n      > SELECT reverse('Spark SQL');\n       LQS krapS\n      > SELECT reverse(array(2, 1, 4, 3));\n       [3,4,1,2]\n  \n    Note:\n      Reverse logic for arrays is available since 2.4.0.\n\n    Since: 1.5.0\n"
        },
        {
            "name": "right",
            "description": "Function: right\nClass: org.apache.spark.sql.catalyst.expressions.Right\nUsage: right(str, len) - Returns the rightmost `len`(`len` can be string type) characters from the string `str`,if `len` is less or equal than 0 the result is an empty string.\nExtended Usage:\n    Examples:\n      > SELECT right('Spark SQL', 3);\n       SQL\n  \n    Since: 2.3.0\n"
        },
        {
            "name": "rint",
            "description": "Function: rint\nClass: org.apache.spark.sql.catalyst.expressions.Rint\nUsage: rint(expr) - Returns the double value that is closest in value to the argument and is equal to a mathematical integer.\nExtended Usage:\n    Examples:\n      > SELECT rint(12.3456);\n       12.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "rlike",
            "description": "Function: rlike\nClass: org.apache.spark.sql.catalyst.expressions.RLike\nUsage: str rlike regexp - Returns true if `str` matches `regexp`, or false otherwise.\nExtended Usage:\n    Arguments:\n      * str - a string expression\n      * regexp - a string expression. The regex string should be a Java regular expression.\n\n          Since Spark 2.0, string literals (including regex patterns) are unescaped in our SQL\n          parser. For example, to match \"\\abc\", a regular expression for `regexp` can be\n          \"^\\\\abc$\".\n\n          There is a SQL config 'spark.sql.parser.escapedStringLiterals' that can be used to\n          fallback to the Spark 1.6 behavior regarding string literal parsing. For example,\n          if the config is enabled, the `regexp` that can match \"\\abc\" is \"^\\abc$\".\n  \n    Examples:\n      > SET spark.sql.parser.escapedStringLiterals=true;\n      spark.sql.parser.escapedStringLiterals\ttrue\n      > SELECT '%SystemDrive%\\Users\\John' rlike '%SystemDrive%\\\\Users.*';\n      true\n      > SET spark.sql.parser.escapedStringLiterals=false;\n      spark.sql.parser.escapedStringLiterals\tfalse\n      > SELECT '%SystemDrive%\\\\Users\\\\John' rlike '%SystemDrive%\\\\\\\\Users.*';\n      true\n  \n    Note:\n      Use LIKE to match with simple string pattern.\n\n    Since: 1.0.0\n"
        },
        {
            "name": "rollup",
            "description": "Function: rollup\nClass: org.apache.spark.sql.catalyst.expressions.Rollup\nUsage: \n    rollup([col1[, col2 ..]]) - create a multi-dimensional rollup using the specified columns\n      so that we can run aggregation on them.\n  \nExtended Usage:\n    Examples:\n      > SELECT name, age, count(*) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY rollup(name, age);\n        Bob\t5\t1\n        Alice\t2\t1\n        Alice\tNULL\t1\n        NULL\tNULL\t2\n        Bob\tNULL\t1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "round",
            "description": "Function: round\nClass: org.apache.spark.sql.catalyst.expressions.Round\nUsage: round(expr, d) - Returns `expr` rounded to `d` decimal places using HALF_UP rounding mode.\nExtended Usage:\n    Examples:\n      > SELECT round(2.5, 0);\n       3\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "row_number",
            "description": "Function: row_number\nClass: org.apache.spark.sql.catalyst.expressions.RowNumber\nUsage: \n    row_number() - Assigns a unique, sequential number to each row, starting with one,\n      according to the ordering of rows within the window partition.\n  \nExtended Usage:\n    Examples:\n      > SELECT a, b, row_number() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);\n       A1\t1\t1\n       A1\t1\t2\n       A1\t2\t3\n       A2\t3\t1\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "rpad",
            "description": "Function: rpad\nClass: org.apache.spark.sql.catalyst.expressions.StringRPad\nUsage: \n    rpad(str, len[, pad]) - Returns `str`, right-padded with `pad` to a length of `len`.\n      If `str` is longer than `len`, the return value is shortened to `len` characters.\n      If `pad` is not specified, `str` will be padded to the right with space characters.\n  \nExtended Usage:\n    Examples:\n      > SELECT rpad('hi', 5, '??');\n       hi???\n      > SELECT rpad('hi', 1, '??');\n       h\n      > SELECT rpad('hi', 5);\n       hi\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "rtrim",
            "description": "Function: rtrim\nClass: org.apache.spark.sql.catalyst.expressions.StringTrimRight\nUsage: \n    rtrim(str) - Removes the trailing space characters from `str`.\n  \nExtended Usage:\n    Arguments:\n      * str - a string expression\n      * trimStr - the trim string characters to trim, the default value is a single space\n  \n    Examples:\n      > SELECT rtrim('    SparkSQL   ');\n       SparkSQL\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "schema_of_csv",
            "description": "Function: schema_of_csv\nClass: org.apache.spark.sql.catalyst.expressions.SchemaOfCsv\nUsage: schema_of_csv(csv[, options]) - Returns schema in the DDL format of CSV string.\nExtended Usage:\n    Examples:\n      > SELECT schema_of_csv('1,abc');\n       STRUCT<`_c0`: INT, `_c1`: STRING>\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "schema_of_json",
            "description": "Function: schema_of_json\nClass: org.apache.spark.sql.catalyst.expressions.SchemaOfJson\nUsage: schema_of_json(json[, options]) - Returns schema in the DDL format of JSON string.\nExtended Usage:\n    Examples:\n      > SELECT schema_of_json('[{\"col\":0}]');\n       ARRAY<STRUCT<`col`: BIGINT>>\n      > SELECT schema_of_json('[{\"col\":01}]', map('allowNumericLeadingZeros', 'true'));\n       ARRAY<STRUCT<`col`: BIGINT>>\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "second",
            "description": "Function: second\nClass: org.apache.spark.sql.catalyst.expressions.Second\nUsage: second(timestamp) - Returns the second component of the string/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT second('2009-07-30 12:58:59');\n       59\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "sentences",
            "description": "Function: sentences\nClass: org.apache.spark.sql.catalyst.expressions.Sentences\nUsage: sentences(str[, lang, country]) - Splits `str` into an array of array of words.\nExtended Usage:\n    Examples:\n      > SELECT sentences('Hi there! Good morning.');\n       [[\"Hi\",\"there\"],[\"Good\",\"morning\"]]\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "sequence",
            "description": "Function: sequence\nClass: org.apache.spark.sql.catalyst.expressions.Sequence\nUsage: \n    sequence(start, stop, step) - Generates an array of elements from start to stop (inclusive),\n      incrementing by step. The type of the returned elements is the same as the type of argument\n      expressions.\n\n      Supported types are: byte, short, integer, long, date, timestamp.\n\n      The start and stop expressions must resolve to the same type.\n      If start and stop expressions resolve to the 'date' or 'timestamp' type\n      then the step expression must resolve to the 'interval' type, otherwise to the same type\n      as the start and stop expressions.\n  \nExtended Usage:\n    Arguments:\n      * start - an expression. The start of the range.\n      * stop - an expression. The end the range (inclusive).\n      * step - an optional expression. The step of the range.\n          By default step is 1 if start is less than or equal to stop, otherwise -1.\n          For the temporal sequences it's 1 day and -1 day respectively.\n          If start is greater than stop then the step must be negative, and vice versa.\n  \n    Examples:\n      > SELECT sequence(1, 5);\n       [1,2,3,4,5]\n      > SELECT sequence(5, 1);\n       [5,4,3,2,1]\n      > SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);\n       [2018-01-01,2018-02-01,2018-03-01]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "sha",
            "description": "Function: sha\nClass: org.apache.spark.sql.catalyst.expressions.Sha1\nUsage: sha(expr) - Returns a sha1 hash value as a hex string of the `expr`.\nExtended Usage:\n    Examples:\n      > SELECT sha('Spark');\n       85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "sha1",
            "description": "Function: sha1\nClass: org.apache.spark.sql.catalyst.expressions.Sha1\nUsage: sha1(expr) - Returns a sha1 hash value as a hex string of the `expr`.\nExtended Usage:\n    Examples:\n      > SELECT sha1('Spark');\n       85f5955f4b27a9a4c2aab6ffe5d7189fc298b92c\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "sha2",
            "description": "Function: sha2\nClass: org.apache.spark.sql.catalyst.expressions.Sha2\nUsage: \n    sha2(expr, bitLength) - Returns a checksum of SHA-2 family as a hex string of `expr`.\n      SHA-224, SHA-256, SHA-384, and SHA-512 are supported. Bit length of 0 is equivalent to 256.\n  \nExtended Usage:\n    Examples:\n      > SELECT sha2('Spark', 256);\n       529bc3b07127ecb7e53a4dcf1991d9152c24537d919178022b2c42657f79a26b\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "shiftleft",
            "description": "Function: shiftleft\nClass: org.apache.spark.sql.catalyst.expressions.ShiftLeft\nUsage: shiftleft(base, expr) - Bitwise left shift.\nExtended Usage:\n    Examples:\n      > SELECT shiftleft(2, 1);\n       4\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "shiftright",
            "description": "Function: shiftright\nClass: org.apache.spark.sql.catalyst.expressions.ShiftRight\nUsage: shiftright(base, expr) - Bitwise (signed) right shift.\nExtended Usage:\n    Examples:\n      > SELECT shiftright(4, 1);\n       2\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "shiftrightunsigned",
            "description": "Function: shiftrightunsigned\nClass: org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned\nUsage: shiftrightunsigned(base, expr) - Bitwise unsigned right shift.\nExtended Usage:\n    Examples:\n      > SELECT shiftrightunsigned(4, 1);\n       2\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "shuffle",
            "description": "Function: shuffle\nClass: org.apache.spark.sql.catalyst.expressions.Shuffle\nUsage: shuffle(array) - Returns a random permutation of the given array.\nExtended Usage:\n    Examples:\n      > SELECT shuffle(array(1, 20, 3, 5));\n       [3,1,5,20]\n      > SELECT shuffle(array(1, 20, null, 3));\n       [20,null,3,1]\n  \n    Note:\n      The function is non-deterministic.\n\n    Since: 2.4.0\n"
        },
        {
            "name": "sign",
            "description": "Function: sign\nClass: org.apache.spark.sql.catalyst.expressions.Signum\nUsage: sign(expr) - Returns -1.0, 0.0 or 1.0 as `expr` is negative, 0 or positive.\nExtended Usage:\n    Examples:\n      > SELECT sign(40);\n       1.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "signum",
            "description": "Function: signum\nClass: org.apache.spark.sql.catalyst.expressions.Signum\nUsage: signum(expr) - Returns -1.0, 0.0 or 1.0 as `expr` is negative, 0 or positive.\nExtended Usage:\n    Examples:\n      > SELECT signum(40);\n       1.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "sin",
            "description": "Function: sin\nClass: org.apache.spark.sql.catalyst.expressions.Sin\nUsage: sin(expr) - Returns the sine of `expr`, as if computed by `java.lang.Math.sin`.\nExtended Usage:\n    Arguments:\n      * expr - angle in radians\n  \n    Examples:\n      > SELECT sin(0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "sinh",
            "description": "Function: sinh\nClass: org.apache.spark.sql.catalyst.expressions.Sinh\nUsage: \n    sinh(expr) - Returns hyperbolic sine of `expr`, as if computed by `java.lang.Math.sinh`.\n  \nExtended Usage:\n    Arguments:\n      * expr - hyperbolic angle\n  \n    Examples:\n      > SELECT sinh(0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "size",
            "description": "Function: size\nClass: org.apache.spark.sql.catalyst.expressions.Size\nUsage: \n    size(expr) - Returns the size of an array or a map.\n    The function returns null for null input if spark.sql.legacy.sizeOfNull is set to false or\n    spark.sql.ansi.enabled is set to true. Otherwise, the function returns -1 for null input.\n    With the default settings, the function returns -1 for null input.\n  \nExtended Usage:\n    Examples:\n      > SELECT size(array('b', 'd', 'c', 'a'));\n       4\n      > SELECT size(map('a', 1, 'b', 2));\n       2\n      > SELECT size(NULL);\n       -1\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "skewness",
            "description": "Function: skewness\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Skewness\nUsage: skewness(expr) - Returns the skewness value calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT skewness(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);\n       1.1135657469022011\n      > SELECT skewness(col) FROM VALUES (-1000), (-100), (10), (20) AS tab(col);\n       -1.1135657469022011\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "slice",
            "description": "Function: slice\nClass: org.apache.spark.sql.catalyst.expressions.Slice\nUsage: slice(x, start, length) - Subsets array x starting from index start (array indices start at 1, or starting from the end if start is negative) with the specified length.\nExtended Usage:\n    Examples:\n      > SELECT slice(array(1, 2, 3, 4), 2, 2);\n       [2,3]\n      > SELECT slice(array(1, 2, 3, 4), -2, 2);\n       [3,4]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "smallint",
            "description": "Function: smallint\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: smallint(expr) - Casts the value `expr` to the target data type `smallint`.\nExtended Usage:\n    No example/argument for smallint.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "some",
            "description": "Function: some\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr\nUsage: some(expr) - Returns true if at least one value of `expr` is true.\nExtended Usage:\n    Examples:\n      > SELECT some(col) FROM VALUES (true), (false), (false) AS tab(col);\n       true\n      > SELECT some(col) FROM VALUES (NULL), (true), (false) AS tab(col);\n       true\n      > SELECT some(col) FROM VALUES (false), (false), (NULL) AS tab(col);\n       false\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "sort_array",
            "description": "Function: sort_array\nClass: org.apache.spark.sql.catalyst.expressions.SortArray\nUsage: \n    sort_array(array[, ascendingOrder]) - Sorts the input array in ascending or descending order\n      according to the natural ordering of the array elements. Null elements will be placed\n      at the beginning of the returned array in ascending order or at the end of the returned\n      array in descending order.\n  \nExtended Usage:\n    Examples:\n      > SELECT sort_array(array('b', 'd', null, 'c', 'a'), true);\n       [null,\"a\",\"b\",\"c\",\"d\"]\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "soundex",
            "description": "Function: soundex\nClass: org.apache.spark.sql.catalyst.expressions.SoundEx\nUsage: soundex(str) - Returns Soundex code of the string.\nExtended Usage:\n    Examples:\n      > SELECT soundex('Miller');\n       M460\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "space",
            "description": "Function: space\nClass: org.apache.spark.sql.catalyst.expressions.StringSpace\nUsage: space(n) - Returns a string consisting of `n` spaces.\nExtended Usage:\n    Examples:\n      > SELECT concat(space(2), '1');\n         1\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "spark_partition_id",
            "description": "Function: spark_partition_id\nClass: org.apache.spark.sql.catalyst.expressions.SparkPartitionID\nUsage: spark_partition_id() - Returns the current partition id.\nExtended Usage:\n    Examples:\n      > SELECT spark_partition_id();\n       0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "split",
            "description": "Function: split\nClass: org.apache.spark.sql.catalyst.expressions.StringSplit\nUsage: split(str, regex, limit) - Splits `str` around occurrences that match `regex` and returns an array with a length of at most `limit`\nExtended Usage:\n    Arguments:\n      * str - a string expression to split.\n      * regex - a string representing a regular expression. The regex string should be a\n        Java regular expression.\n      * limit - an integer expression which controls the number of times the regex is applied.\n          * limit > 0: The resulting array's length will not be more than `limit`,\n            and the resulting array's last entry will contain all input\n            beyond the last matched regex.\n          * limit <= 0: `regex` will be applied as many times as possible, and\n            the resulting array can be of any size.\n  \n    Examples:\n      > SELECT split('oneAtwoBthreeC', '[ABC]');\n       [\"one\",\"two\",\"three\",\"\"]\n      > SELECT split('oneAtwoBthreeC', '[ABC]', -1);\n       [\"one\",\"two\",\"three\",\"\"]\n      > SELECT split('oneAtwoBthreeC', '[ABC]', 2);\n       [\"one\",\"twoBthreeC\"]\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "sqrt",
            "description": "Function: sqrt\nClass: org.apache.spark.sql.catalyst.expressions.Sqrt\nUsage: sqrt(expr) - Returns the square root of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT sqrt(4);\n       2.0\n  \n    Since: 1.1.1\n"
        },
        {
            "name": "stack",
            "description": "Function: stack\nClass: org.apache.spark.sql.catalyst.expressions.Stack\nUsage: stack(n, expr1, ..., exprk) - Separates `expr1`, ..., `exprk` into `n` rows. Uses column names col0, col1, etc. by default unless specified otherwise.\nExtended Usage:\n    Examples:\n      > SELECT stack(2, 1, 2, 3);\n       1\t2\n       3\tNULL\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "std",
            "description": "Function: std\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp\nUsage: std(expr) - Returns the sample standard deviation calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT std(col) FROM VALUES (1), (2), (3) AS tab(col);\n       1.0\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "stddev",
            "description": "Function: stddev\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp\nUsage: stddev(expr) - Returns the sample standard deviation calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT stddev(col) FROM VALUES (1), (2), (3) AS tab(col);\n       1.0\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "stddev_pop",
            "description": "Function: stddev_pop\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop\nUsage: stddev_pop(expr) - Returns the population standard deviation calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT stddev_pop(col) FROM VALUES (1), (2), (3) AS tab(col);\n       0.816496580927726\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "stddev_samp",
            "description": "Function: stddev_samp\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp\nUsage: stddev_samp(expr) - Returns the sample standard deviation calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT stddev_samp(col) FROM VALUES (1), (2), (3) AS tab(col);\n       1.0\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "str_to_map",
            "description": "Function: str_to_map\nClass: org.apache.spark.sql.catalyst.expressions.StringToMap\nUsage: str_to_map(text[, pairDelim[, keyValueDelim]]) - Creates a map after splitting the text into key/value pairs using delimiters. Default delimiters are ',' for `pairDelim` and ':' for `keyValueDelim`. Both `pairDelim` and `keyValueDelim` are treated as regular expressions.\nExtended Usage:\n    Examples:\n      > SELECT str_to_map('a:1,b:2,c:3', ',', ':');\n       {\"a\":\"1\",\"b\":\"2\",\"c\":\"3\"}\n      > SELECT str_to_map('a');\n       {\"a\":null}\n  \n    Since: 2.0.1\n"
        },
        {
            "name": "string",
            "description": "Function: string\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: string(expr) - Casts the value `expr` to the target data type `string`.\nExtended Usage:\n    No example/argument for string.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "struct",
            "description": "Function: struct\nClass: org.apache.spark.sql.catalyst.expressions.CreateNamedStruct\nUsage: struct(col1, col2, col3, ...) - Creates a struct with the given field values.\nExtended Usage:\n    Examples:\n      > SELECT struct(1, 2, 3);\n       {\"col1\":1,\"col2\":2,\"col3\":3}\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "substr",
            "description": "Function: substr\nClass: org.apache.spark.sql.catalyst.expressions.Substring\nUsage: \n    substr(str, pos[, len]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\n\n    substr(str FROM pos[ FOR len]]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\n  \nExtended Usage:\n    Examples:\n      > SELECT substr('Spark SQL', 5);\n       k SQL\n      > SELECT substr('Spark SQL', -3);\n       SQL\n      > SELECT substr('Spark SQL', 5, 1);\n       k\n      > SELECT substr('Spark SQL' FROM 5);\n       k SQL\n      > SELECT substr('Spark SQL' FROM -3);\n       SQL\n      > SELECT substr('Spark SQL' FROM 5 FOR 1);\n       k\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "substring",
            "description": "Function: substring\nClass: org.apache.spark.sql.catalyst.expressions.Substring\nUsage: \n    substring(str, pos[, len]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\n\n    substring(str FROM pos[ FOR len]]) - Returns the substring of `str` that starts at `pos` and is of length `len`, or the slice of byte array that starts at `pos` and is of length `len`.\n  \nExtended Usage:\n    Examples:\n      > SELECT substring('Spark SQL', 5);\n       k SQL\n      > SELECT substring('Spark SQL', -3);\n       SQL\n      > SELECT substring('Spark SQL', 5, 1);\n       k\n      > SELECT substring('Spark SQL' FROM 5);\n       k SQL\n      > SELECT substring('Spark SQL' FROM -3);\n       SQL\n      > SELECT substring('Spark SQL' FROM 5 FOR 1);\n       k\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "substring_index",
            "description": "Function: substring_index\nClass: org.apache.spark.sql.catalyst.expressions.SubstringIndex\nUsage: \n    substring_index(str, delim, count) - Returns the substring from `str` before `count` occurrences of the delimiter `delim`.\n      If `count` is positive, everything to the left of the final delimiter (counting from the\n      left) is returned. If `count` is negative, everything to the right of the final delimiter\n      (counting from the right) is returned. The function substring_index performs a case-sensitive match\n      when searching for `delim`.\n  \nExtended Usage:\n    Examples:\n      > SELECT substring_index('www.apache.org', '.', 2);\n       www.apache\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "sum",
            "description": "Function: sum\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.Sum\nUsage: sum(expr) - Returns the sum calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT sum(col) FROM VALUES (5), (10), (15) AS tab(col);\n       30\n      > SELECT sum(col) FROM VALUES (NULL), (10), (15) AS tab(col);\n       25\n      > SELECT sum(col) FROM VALUES (NULL), (NULL) AS tab(col);\n       NULL\n  \n    Since: 1.0.0\n"
        },
        {
            "name": "tan",
            "description": "Function: tan\nClass: org.apache.spark.sql.catalyst.expressions.Tan\nUsage: \n    tan(expr) - Returns the tangent of `expr`, as if computed by `java.lang.Math.tan`.\n  \nExtended Usage:\n    Arguments:\n      * expr - angle in radians\n  \n    Examples:\n      > SELECT tan(0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "tanh",
            "description": "Function: tanh\nClass: org.apache.spark.sql.catalyst.expressions.Tanh\nUsage: \n    tanh(expr) - Returns the hyperbolic tangent of `expr`, as if computed by\n      `java.lang.Math.tanh`.\n  \nExtended Usage:\n    Arguments:\n      * expr - hyperbolic angle\n  \n    Examples:\n      > SELECT tanh(0);\n       0.0\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "timestamp",
            "description": "Function: timestamp\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: timestamp(expr) - Casts the value `expr` to the target data type `timestamp`.\nExtended Usage:\n    No example/argument for timestamp.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "timestamp_micros",
            "description": "Function: timestamp_micros\nClass: org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp\nUsage: timestamp_micros(microseconds) - Creates timestamp from the number of microseconds since UTC epoch.\nExtended Usage:\n    Examples:\n      > SELECT timestamp_micros(1230219000123123);\n       2008-12-25 07:30:00.123123\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "timestamp_millis",
            "description": "Function: timestamp_millis\nClass: org.apache.spark.sql.catalyst.expressions.MillisToTimestamp\nUsage: timestamp_millis(milliseconds) - Creates timestamp from the number of milliseconds since UTC epoch.\nExtended Usage:\n    Examples:\n      > SELECT timestamp_millis(1230219000123);\n       2008-12-25 07:30:00.123\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "timestamp_seconds",
            "description": "Function: timestamp_seconds\nClass: org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp\nUsage: timestamp_seconds(seconds) - Creates timestamp from the number of seconds (can be fractional) since UTC epoch.\nExtended Usage:\n    Examples:\n      > SELECT timestamp_seconds(1230219000);\n       2008-12-25 07:30:00\n      > SELECT timestamp_seconds(1230219000.123);\n       2008-12-25 07:30:00.123\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "tinyint",
            "description": "Function: tinyint\nClass: org.apache.spark.sql.catalyst.expressions.Cast\nUsage: tinyint(expr) - Casts the value `expr` to the target data type `tinyint`.\nExtended Usage:\n    No example/argument for tinyint.\n\n    Since: 2.0.1\n"
        },
        {
            "name": "to_csv",
            "description": "Function: to_csv\nClass: org.apache.spark.sql.catalyst.expressions.StructsToCsv\nUsage: to_csv(expr[, options]) - Returns a CSV string with a given struct value\nExtended Usage:\n    Examples:\n      > SELECT to_csv(named_struct('a', 1, 'b', 2));\n       1,2\n      > SELECT to_csv(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));\n       26/08/2015\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "to_date",
            "description": "Function: to_date\nClass: org.apache.spark.sql.catalyst.expressions.ParseToDate\nUsage: \n    to_date(date_str[, fmt]) - Parses the `date_str` expression with the `fmt` expression to\n      a date. Returns null with invalid input. By default, it follows casting rules to a date if\n      the `fmt` is omitted.\n  \nExtended Usage:\n    Arguments:\n      * date_str - A string to be parsed to date.\n      * fmt - Date format pattern to follow. See <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\">Datetime Patterns</a> for valid\n              date and time format patterns.\n  \n    Examples:\n      > SELECT to_date('2009-07-30 04:17:52');\n       2009-07-30\n      > SELECT to_date('2016-12-31', 'yyyy-MM-dd');\n       2016-12-31\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "to_json",
            "description": "Function: to_json\nClass: org.apache.spark.sql.catalyst.expressions.StructsToJson\nUsage: to_json(expr[, options]) - Returns a JSON string with a given struct value\nExtended Usage:\n    Examples:\n      > SELECT to_json(named_struct('a', 1, 'b', 2));\n       {\"a\":1,\"b\":2}\n      > SELECT to_json(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));\n       {\"time\":\"26/08/2015\"}\n      > SELECT to_json(array(named_struct('a', 1, 'b', 2)));\n       [{\"a\":1,\"b\":2}]\n      > SELECT to_json(map('a', named_struct('b', 1)));\n       {\"a\":{\"b\":1}}\n      > SELECT to_json(map(named_struct('a', 1),named_struct('b', 2)));\n       {\"[1]\":{\"b\":2}}\n      > SELECT to_json(map('a', 1));\n       {\"a\":1}\n      > SELECT to_json(array((map('a', 1))));\n       [{\"a\":1}]\n  \n    Since: 2.2.0\n"
        },
        {
            "name": "to_timestamp",
            "description": "Function: to_timestamp\nClass: org.apache.spark.sql.catalyst.expressions.ParseToTimestamp\nUsage: \n    to_timestamp(timestamp_str[, fmt]) - Parses the `timestamp_str` expression with the `fmt` expression\n      to a timestamp. Returns null with invalid input. By default, it follows casting rules to\n      a timestamp if the `fmt` is omitted.\n  \nExtended Usage:\n    Arguments:\n      * timestamp_str - A string to be parsed to timestamp.\n      * fmt - Timestamp format pattern to follow. See <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\">Datetime Patterns</a> for valid\n              date and time format patterns.\n  \n    Examples:\n      > SELECT to_timestamp('2016-12-31 00:12:00');\n       2016-12-31 00:12:00\n      > SELECT to_timestamp('2016-12-31', 'yyyy-MM-dd');\n       2016-12-31 00:00:00\n  \n    Since: 2.2.0\n"
        },
        {
            "name": "to_unix_timestamp",
            "description": "Function: to_unix_timestamp\nClass: org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp\nUsage: to_unix_timestamp(timeExp[, fmt]) - Returns the UNIX timestamp of the given time.\nExtended Usage:\n    Arguments:\n      * timeExp - A date/timestamp or string which is returned as a UNIX timestamp.\n      * fmt - Date/time format pattern to follow. Ignored if `timeExp` is not a string.\n              Default value is \"yyyy-MM-dd HH:mm:ss\". See <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\">Datetime Patterns</a>\n              for valid date and time format patterns.\n  \n    Examples:\n      > SELECT to_unix_timestamp('2016-04-08', 'yyyy-MM-dd');\n       1460098800\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "to_utc_timestamp",
            "description": "Function: to_utc_timestamp\nClass: org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp\nUsage: to_utc_timestamp(timestamp, timezone) - Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield '2017-07-14 01:40:00.0'.\nExtended Usage:\n    Examples:\n      > SELECT to_utc_timestamp('2016-08-31', 'Asia/Seoul');\n       2016-08-30 15:00:00\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "transform",
            "description": "Function: transform\nClass: org.apache.spark.sql.catalyst.expressions.ArrayTransform\nUsage: transform(expr, func) - Transforms elements in an array using the function.\nExtended Usage:\n    Examples:\n      > SELECT transform(array(1, 2, 3), x -> x + 1);\n       [2,3,4]\n      > SELECT transform(array(1, 2, 3), (x, i) -> x + i);\n       [1,3,5]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "transform_keys",
            "description": "Function: transform_keys\nClass: org.apache.spark.sql.catalyst.expressions.TransformKeys\nUsage: transform_keys(expr, func) - Transforms elements in a map using the function.\nExtended Usage:\n    Examples:\n      > SELECT transform_keys(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + 1);\n       {2:1,3:2,4:3}\n      > SELECT transform_keys(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + v);\n       {2:1,4:2,6:3}\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "transform_values",
            "description": "Function: transform_values\nClass: org.apache.spark.sql.catalyst.expressions.TransformValues\nUsage: transform_values(expr, func) - Transforms values in the map using the function.\nExtended Usage:\n    Examples:\n      > SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> v + 1);\n       {1:2,2:3,3:4}\n      > SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + v);\n       {1:2,2:4,3:6}\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "translate",
            "description": "Function: translate\nClass: org.apache.spark.sql.catalyst.expressions.StringTranslate\nUsage: translate(input, from, to) - Translates the `input` string by replacing the characters present in the `from` string with the corresponding characters in the `to` string.\nExtended Usage:\n    Examples:\n      > SELECT translate('AaBbCc', 'abc', '123');\n       A1B2C3\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "trim",
            "description": "Function: trim\nClass: org.apache.spark.sql.catalyst.expressions.StringTrim\nUsage: \n    trim(str) - Removes the leading and trailing space characters from `str`.\n\n    trim(BOTH FROM str) - Removes the leading and trailing space characters from `str`.\n\n    trim(LEADING FROM str) - Removes the leading space characters from `str`.\n\n    trim(TRAILING FROM str) - Removes the trailing space characters from `str`.\n\n    trim(trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n\n    trim(BOTH trimStr FROM str) - Remove the leading and trailing `trimStr` characters from `str`.\n\n    trim(LEADING trimStr FROM str) - Remove the leading `trimStr` characters from `str`.\n\n    trim(TRAILING trimStr FROM str) - Remove the trailing `trimStr` characters from `str`.\n  \nExtended Usage:\n    Arguments:\n      * str - a string expression\n      * trimStr - the trim string characters to trim, the default value is a single space\n      * BOTH, FROM - these are keywords to specify trimming string characters from both ends of\n          the string\n      * LEADING, FROM - these are keywords to specify trimming string characters from the left\n          end of the string\n      * TRAILING, FROM - these are keywords to specify trimming string characters from the right\n          end of the string\n  \n    Examples:\n      > SELECT trim('    SparkSQL   ');\n       SparkSQL\n      > SELECT trim(BOTH FROM '    SparkSQL   ');\n       SparkSQL\n      > SELECT trim(LEADING FROM '    SparkSQL   ');\n       SparkSQL\n      > SELECT trim(TRAILING FROM '    SparkSQL   ');\n           SparkSQL\n      > SELECT trim('SL' FROM 'SSparkSQLS');\n       parkSQ\n      > SELECT trim(BOTH 'SL' FROM 'SSparkSQLS');\n       parkSQ\n      > SELECT trim(LEADING 'SL' FROM 'SSparkSQLS');\n       parkSQLS\n      > SELECT trim(TRAILING 'SL' FROM 'SSparkSQLS');\n       SSparkSQ\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "trunc",
            "description": "Function: trunc\nClass: org.apache.spark.sql.catalyst.expressions.TruncDate\nUsage: \n    trunc(date, fmt) - Returns `date` with the time portion of the day truncated to the unit specified by the format model `fmt`.\n  \nExtended Usage:\n    Arguments:\n      * date - date value or valid date string\n      * fmt - the format representing the unit to be truncated to\n          - \"YEAR\", \"YYYY\", \"YY\" - truncate to the first date of the year that the `date` falls in\n          - \"QUARTER\" - truncate to the first date of the quarter that the `date` falls in\n          - \"MONTH\", \"MM\", \"MON\" - truncate to the first date of the month that the `date` falls in\n          - \"WEEK\" - truncate to the Monday of the week that the `date` falls in\n  \n    Examples:\n      > SELECT trunc('2019-08-04', 'week');\n       2019-07-29\n      > SELECT trunc('2019-08-04', 'quarter');\n       2019-07-01\n      > SELECT trunc('2009-02-12', 'MM');\n       2009-02-01\n      > SELECT trunc('2015-10-27', 'YEAR');\n       2015-01-01\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "typeof",
            "description": "Function: typeof\nClass: org.apache.spark.sql.catalyst.expressions.TypeOf\nUsage: typeof(expr) - Return DDL-formatted type string for the data type of the input.\nExtended Usage:\n    Examples:\n      > SELECT typeof(1);\n       int\n      > SELECT typeof(array(1));\n       array<int>\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "ucase",
            "description": "Function: ucase\nClass: org.apache.spark.sql.catalyst.expressions.Upper\nUsage: ucase(str) - Returns `str` with all characters changed to uppercase.\nExtended Usage:\n    Examples:\n      > SELECT ucase('SparkSql');\n       SPARKSQL\n  \n    Since: 1.0.1\n"
        },
        {
            "name": "unbase64",
            "description": "Function: unbase64\nClass: org.apache.spark.sql.catalyst.expressions.UnBase64\nUsage: unbase64(str) - Converts the argument from a base 64 string `str` to a binary.\nExtended Usage:\n    Examples:\n      > SELECT unbase64('U3BhcmsgU1FM');\n       Spark SQL\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "unhex",
            "description": "Function: unhex\nClass: org.apache.spark.sql.catalyst.expressions.Unhex\nUsage: unhex(expr) - Converts hexadecimal `expr` to binary.\nExtended Usage:\n    Examples:\n      > SELECT decode(unhex('537061726B2053514C'), 'UTF-8');\n       Spark SQL\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "unix_date",
            "description": "Function: unix_date\nClass: org.apache.spark.sql.catalyst.expressions.UnixDate\nUsage: unix_date(date) - Returns the number of days since 1970-01-01.\nExtended Usage:\n    Examples:\n      > SELECT unix_date(DATE(\"1970-01-02\"));\n       1\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "unix_micros",
            "description": "Function: unix_micros\nClass: org.apache.spark.sql.catalyst.expressions.UnixMicros\nUsage: unix_micros(timestamp) - Returns the number of microseconds since 1970-01-01 00:00:00 UTC.\nExtended Usage:\n    Examples:\n      > SELECT unix_micros(TIMESTAMP('1970-01-01 00:00:01Z'));\n       1000000\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "unix_millis",
            "description": "Function: unix_millis\nClass: org.apache.spark.sql.catalyst.expressions.UnixMillis\nUsage: unix_millis(timestamp) - Returns the number of milliseconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.\nExtended Usage:\n    Examples:\n      > SELECT unix_millis(TIMESTAMP('1970-01-01 00:00:01Z'));\n       1000\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "unix_seconds",
            "description": "Function: unix_seconds\nClass: org.apache.spark.sql.catalyst.expressions.UnixSeconds\nUsage: unix_seconds(timestamp) - Returns the number of seconds since 1970-01-01 00:00:00 UTC. Truncates higher levels of precision.\nExtended Usage:\n    Examples:\n      > SELECT unix_seconds(TIMESTAMP('1970-01-01 00:00:01Z'));\n       1\n  \n    Since: 3.1.0\n"
        },
        {
            "name": "unix_timestamp",
            "description": "Function: unix_timestamp\nClass: org.apache.spark.sql.catalyst.expressions.UnixTimestamp\nUsage: unix_timestamp([timeExp[, fmt]]) - Returns the UNIX timestamp of current or specified time.\nExtended Usage:\n    Arguments:\n      * timeExp - A date/timestamp or string. If not provided, this defaults to current time.\n      * fmt - Date/time format pattern to follow. Ignored if `timeExp` is not a string.\n              Default value is \"yyyy-MM-dd HH:mm:ss\". See <a href=\"https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\"> Datetime Patterns</a>\n              for valid date and time format patterns.\n  \n    Examples:\n      > SELECT unix_timestamp();\n       1476884637\n      > SELECT unix_timestamp('2016-04-08', 'yyyy-MM-dd');\n       1460041200\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "upper",
            "description": "Function: upper\nClass: org.apache.spark.sql.catalyst.expressions.Upper\nUsage: upper(str) - Returns `str` with all characters changed to uppercase.\nExtended Usage:\n    Examples:\n      > SELECT upper('SparkSql');\n       SPARKSQL\n  \n    Since: 1.0.1\n"
        },
        {
            "name": "uuid",
            "description": "Function: uuid\nClass: org.apache.spark.sql.catalyst.expressions.Uuid\nUsage: uuid() - Returns an universally unique identifier (UUID) string. The value is returned as a canonical UUID 36-character string.\nExtended Usage:\n    Examples:\n      > SELECT uuid();\n       46707d92-02f4-4817-8116-a4c3b23e6266\n  \n    Note:\n      The function is non-deterministic.\n\n    Since: 2.3.0\n"
        },
        {
            "name": "var_pop",
            "description": "Function: var_pop\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop\nUsage: var_pop(expr) - Returns the population variance calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT var_pop(col) FROM VALUES (1), (2), (3) AS tab(col);\n       0.6666666666666666\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "var_samp",
            "description": "Function: var_samp\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp\nUsage: var_samp(expr) - Returns the sample variance calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT var_samp(col) FROM VALUES (1), (2), (3) AS tab(col);\n       1.0\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "variance",
            "description": "Function: variance\nClass: org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp\nUsage: variance(expr) - Returns the sample variance calculated from values of a group.\nExtended Usage:\n    Examples:\n      > SELECT variance(col) FROM VALUES (1), (2), (3) AS tab(col);\n       1.0\n  \n    Since: 1.6.0\n"
        },
        {
            "name": "version",
            "description": "Function: version\nClass: org.apache.spark.sql.catalyst.expressions.SparkVersion\nUsage: version() - Returns the Spark version. The string contains 2 fields, the first being a release version and the second being a git revision.\nExtended Usage:\n    Examples:\n      > SELECT version();\n       3.1.0 a6d6ea3efedbad14d99c24143834cd4e2e52fb40\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "weekday",
            "description": "Function: weekday\nClass: org.apache.spark.sql.catalyst.expressions.WeekDay\nUsage: weekday(date) - Returns the day of the week for date/timestamp (0 = Monday, 1 = Tuesday, ..., 6 = Sunday).\nExtended Usage:\n    Examples:\n      > SELECT weekday('2009-07-30');\n       3\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "weekofyear",
            "description": "Function: weekofyear\nClass: org.apache.spark.sql.catalyst.expressions.WeekOfYear\nUsage: weekofyear(date) - Returns the week of the year of the given date. A week is considered to start on a Monday and week 1 is the first week with >3 days.\nExtended Usage:\n    Examples:\n      > SELECT weekofyear('2008-02-20');\n       8\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "when",
            "description": "Function: when\nClass: org.apache.spark.sql.catalyst.expressions.CaseWhen\nUsage: CASE WHEN expr1 THEN expr2 [WHEN expr3 THEN expr4]* [ELSE expr5] END - When `expr1` = true, returns `expr2`; else when `expr3` = true, returns `expr4`; else returns `expr5`.\nExtended Usage:\n    Arguments:\n      * expr1, expr3 - the branch condition expressions should all be boolean type.\n      * expr2, expr4, expr5 - the branch value expressions and else value expression should all be\n          same type or coercible to a common type.\n  \n    Examples:\n      > SELECT CASE WHEN 1 > 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;\n       1.0\n      > SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;\n       2.0\n      > SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 < 0 THEN 2.0 END;\n       NULL\n  \n    Since: 1.0.1\n"
        },
        {
            "name": "width_bucket",
            "description": "Function: width_bucket\nClass: org.apache.spark.sql.catalyst.expressions.WidthBucket\nUsage: \n    width_bucket(value, min_value, max_value, num_bucket) - Returns the bucket number to which\n      `value` would be assigned in an equiwidth histogram with `num_bucket` buckets,\n      in the range `min_value` to `max_value`.\"\n  \nExtended Usage:\n    Examples:\n      > SELECT width_bucket(5.3, 0.2, 10.6, 5);\n       3\n      > SELECT width_bucket(-2.1, 1.3, 3.4, 3);\n       0\n      > SELECT width_bucket(8.1, 0.0, 5.7, 4);\n       5\n      > SELECT width_bucket(-0.9, 5.2, 0.5, 2);\n       3\n  \n    Since: 3.1.0\n"
        },

        {
            "name": "window",
            "description": "Function: window\nClass: org.apache.spark.sql.catalyst.expressions.TimeWindow\nUsage: N/A.\nExtended Usage:\n    No example/argument for window.\n"
        },
        {
            "name": "xpath",
            "description": "Function: xpath\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathList\nUsage: xpath(xml, xpath) - Returns a string array of values within the nodes of xml that match the XPath expression.\nExtended Usage:\n    Examples:\n      > SELECT xpath('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>','a/b/text()');\n       [\"b1\",\"b2\",\"b3\"]\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_boolean",
            "description": "Function: xpath_boolean\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean\nUsage: xpath_boolean(xml, xpath) - Returns true if the XPath expression evaluates to true, or if a matching node is found.\nExtended Usage:\n    Examples:\n      > SELECT xpath_boolean('<a><b>1</b></a>','a/b');\n       true\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_double",
            "description": "Function: xpath_double\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathDouble\nUsage: xpath_double(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.\nExtended Usage:\n    Examples:\n      > SELECT xpath_double('<a><b>1</b><b>2</b></a>', 'sum(a/b)');\n       3.0\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_float",
            "description": "Function: xpath_float\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathFloat\nUsage: xpath_float(xml, xpath) - Returns a float value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.\nExtended Usage:\n    Examples:\n      > SELECT xpath_float('<a><b>1</b><b>2</b></a>', 'sum(a/b)');\n       3.0\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_int",
            "description": "Function: xpath_int\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathInt\nUsage: xpath_int(xml, xpath) - Returns an integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.\nExtended Usage:\n    Examples:\n      > SELECT xpath_int('<a><b>1</b><b>2</b></a>', 'sum(a/b)');\n       3\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_long",
            "description": "Function: xpath_long\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathLong\nUsage: xpath_long(xml, xpath) - Returns a long integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.\nExtended Usage:\n    Examples:\n      > SELECT xpath_long('<a><b>1</b><b>2</b></a>', 'sum(a/b)');\n       3\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_number",
            "description": "Function: xpath_number\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathDouble\nUsage: xpath_number(xml, xpath) - Returns a double value, the value zero if no match is found, or NaN if a match is found but the value is non-numeric.\nExtended Usage:\n    Examples:\n      > SELECT xpath_number('<a><b>1</b><b>2</b></a>', 'sum(a/b)');\n       3.0\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_short",
            "description": "Function: xpath_short\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathShort\nUsage: xpath_short(xml, xpath) - Returns a short integer value, or the value zero if no match is found, or a match is found but the value is non-numeric.\nExtended Usage:\n    Examples:\n      > SELECT xpath_short('<a><b>1</b><b>2</b></a>', 'sum(a/b)');\n       3\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xpath_string",
            "description": "Function: xpath_string\nClass: org.apache.spark.sql.catalyst.expressions.xml.XPathString\nUsage: xpath_string(xml, xpath) - Returns the text contents of the first xml node that matches the XPath expression.\nExtended Usage:\n    Examples:\n      > SELECT xpath_string('<a><b>b</b><c>cc</c></a>','a/c');\n       cc\n  \n    Since: 2.0.0\n"
        },
        {
            "name": "xxhash64",
            "description": "Function: xxhash64\nClass: org.apache.spark.sql.catalyst.expressions.XxHash64\nUsage: xxhash64(expr1, expr2, ...) - Returns a 64-bit hash value of the arguments.\nExtended Usage:\n    Examples:\n      > SELECT xxhash64('Spark', array(123), 2);\n       5602566077635097486\n  \n    Since: 3.0.0\n"
        },
        {
            "name": "year",
            "description": "Function: year\nClass: org.apache.spark.sql.catalyst.expressions.Year\nUsage: year(date) - Returns the year component of the date/timestamp.\nExtended Usage:\n    Examples:\n      > SELECT year('2016-07-30');\n       2016\n  \n    Since: 1.5.0\n"
        },
        {
            "name": "zip_with",
            "description": "Function: zip_with\nClass: org.apache.spark.sql.catalyst.expressions.ZipWith\nUsage: zip_with(left, right, func) - Merges the two given arrays, element-wise, into a single array using function. If one array is shorter, nulls are appended at the end to match the length of the longer array, before applying function.\nExtended Usage:\n    Examples:\n      > SELECT zip_with(array(1, 2, 3), array('a', 'b', 'c'), (x, y) -> (y, x));\n       [{\"y\":\"a\",\"x\":1},{\"y\":\"b\",\"x\":2},{\"y\":\"c\",\"x\":3}]\n      > SELECT zip_with(array(1, 2), array(3, 4), (x, y) -> x + y);\n       [4,6]\n      > SELECT zip_with(array('a', 'b', 'c'), array('d', 'e', 'f'), (x, y) -> concat(x, y));\n       [\"ad\",\"be\",\"cf\"]\n  \n    Since: 2.4.0\n"
        },
        {
            "name": "|",
            "description": "Function: |\nClass: org.apache.spark.sql.catalyst.expressions.BitwiseOr\nUsage: expr1 | expr2 - Returns the result of bitwise OR of `expr1` and `expr2`.\nExtended Usage:\n    Examples:\n      > SELECT 3 | 5;\n       7\n  \n    Since: 1.4.0\n"
        },
        {
            "name": "||",
            "description": "Function: ||\nUsage: expr1 || expr2 - Returns the concatenation of `expr1` and `expr2`."
        },
        {
            "name": "~",
            "description": "Function: ~\nClass: org.apache.spark.sql.catalyst.expressions.BitwiseNot\nUsage: ~ expr - Returns the result of bitwise NOT of `expr`.\nExtended Usage:\n    Examples:\n      > SELECT ~ 0;\n       -1\n  \n    Since: 1.4.0\n"
        }
    ],
    
    "tables": [
        {
            "columns": [
              {
                "columnName": "id",
                "description": "integer",
                "metadata": {},
                "type": "integer"
              },
              {
                "columnName": "name",
                "description": "string",
                "metadata": {
                  "description": "new description"
                },
                "type": "string"
              },
              {
                "columnName": "age",
                "description": "integer",
                "metadata": {},
                "type": "integer"
              },
              {
                "columnName": "books",
                "description": "array",
                "metadata": {},
                "type": "array"
              },
              {
                "columnName": "books.title",
                "description": "string",
                "metadata": {},
                "type": "string"
              },
              {
                "columnName": "books.chapters",
                "description": "array",
                "metadata": {},
                "type": "array"
              },
              {
                "columnName": "books.chapters.paragraph",
                "description": "string",
                "metadata": {},
                "type": "string"
              },
              {
                "columnName": "struct_col",
                "description": "struct",
                "metadata": {},
                "type": "struct"
              },
              {
                "columnName": "struct_col.address",
                "description": "struct",
                "metadata": {},
                "type": "struct"
              },
              {
                "columnName": "struct_col.address.streetName",
                "description": "string",
                "metadata": {},
                "type": "string"
              },
              {
                "columnName": "struct_col.address.streetNumber",
                "description": "long",
                "metadata": {},
                "type": "long"
              },
              {
                "columnName": "map_col",
                "description": "map",
                "metadata": {},
                "type": "map"
              },
              {
                "columnName": "map_col.key.key.start",
                "description": "long",
                "metadata": {},
                "type": "long"
              },
              {
                "columnName": "map_col.key.key.start",
                "description": "long",
                "metadata": {},
                "type": "long"
              },
              {
                "columnName": "`spa ces`.`sp aces`",
                "description": "with spaces",
                "metadata": {},
                "type": "long"
              }
            ],
            "database": null,
            "tableName": "student"
          }
      ,
          {
            "database": null,
            "tableName": "nested_table",
            "columns": [
                {
                    "columnName": "col1",
                    "description": "the desc. text"
                },
                {
                    "columnName": "struct_col",
                    "description": "the desc. text"
                },
                {
                    "columnName": "struct_col.name",
                    "description": "the desc. text"
                },
                {
                    "columnName": "array_col",
                    "description": "array"
                }
            ]
        },
        {
            "database": null,
            "tableName": "tbl1",
            "columns": [
                {
                    "columnName": "one",
                    "description": "one(Type: varchar(10), Null: No, Default: null)"
                },
                {
                    "columnName": "two",
                    "description": "two(Type: smallint, Null: No, Default: null)"
                }
            ]
        },
        {
            "database": null,
            "tableName": "contacts",
            "columns": [
                {
                    "columnName": "contact_id",
                    "description": "contact_id(Type: INTEGER, Null: No, Default: null)"
                },
                {
                    "columnName": "first_name",
                    "description": "first_name(Type: TEXT, Null: Yes, Default: null)"
                },
                {
                    "columnName": "last_name",
                    "description": "last_name(Type: TEXT, Null: Yes, Default: null)"
                },
                {
                    "columnName": "email",
                    "description": "email(Type: TEXT, Null: Yes, Default: null)"
                },
                {
                    "columnName": "phone",
                    "description": "phone(Type: TEXT, Null: Yes, Default: null)"
                }
            ]
        },
        {
            "database": null,
            "tableName": "groups",
            "columns": [
                {
                    "columnName": "group_id",
                    "description": "group_id(Type: INTEGER, Null: No, Default: null)"
                },
                {
                    "columnName": "name",
                    "description": "name(Type: TEXT, Null: Yes, Default: null)"
                }
            ]
        },
        {
            "database": null,
            "tableName": "contact_groups",
            "columns": [
                {
                    "columnName": "contact_id",
                    "description": "contact_id(Type: INTEGER, Null: No, Default: null)"
                },
                {
                    "columnName": "group_id",
                    "description": "group_id(Type: INTEGER, Null: No, Default: null)"
                }
            ]
        },
        {
            "database": null,
            "tableName": "COMPANY",
            "columns": [
                {
                    "columnName": "ID",
                    "description": "ID(Type: INT, Null: Yes, Default: null)"
                },
                {
                    "columnName": "NAME",
                    "description": "NAME(Type: TEXT, Null: Yes, Default: null)"
                },
                {
                    "columnName": "AGE",
                    "description": "AGE(Type: INT, Null: Yes, Default: null)"
                },
                {
                    "columnName": "ADDRESS",
                    "description": "ADDRESS(Type: CHAR(50), Null: No, Default: null)"
                },
                {
                    "columnName": "SALARY",
                    "description": "SALARY(Type: REAL, Null: No, Default: null)"
                }
            ]
        },
        {
            "database": null,
            "tableName": "employees",
            "columns": [
                {
                    "columnName": "job_id",
                    "description": ""
                },
                {
                    "columnName": "employee_id",
                    "description": ""
                },
                {
                    "columnName": "manager_id",
                    "description": ""
                },
                {
                    "columnName": "department_id",
                    "description": ""
                },
                {
                    "columnName": "first_name",
                    "description": ""
                },
                {
                    "columnName": "last_name",
                    "description": ""
                },
                {
                    "columnName": "email",
                    "description": ""
                },
                {
                    "columnName": "phone_number",
                    "description": ""
                },
                {
                    "columnName": "hire_date",
                    "description": ""
                },
                {
                    "columnName": "salary",
                    "description": ""
                },
                {
                    "columnName": "commision_pct",
                    "description": ""
                }
            ]
        },
        {
            "database": null,
            "tableName": "jobs",
            "columns": [
                {
                    "columnName": "job_id",
                    "description": ""
                },
                {
                    "columnName": "job_title",
                    "description": ""
                },
                {
                    "columnName": "min_salary",
                    "description": ""
                },
                {
                    "columnName": "max_salary",
                    "description": ""
                },
                {
                    "columnName": "created_at",
                    "description": ""
                },
                {
                    "columnName": "updated_at",
                    "description": ""
                }
            ]
        },
        {
            "database": null,
            "tableName": "job_history",
            "columns": [
                {
                    "columnName": "employee_id",
                    "description": ""
                },
                {
                    "columnName": "start_date",
                    "description": ""
                },
                {
                    "columnName": "end_date",
                    "description": ""
                },
                {
                    "columnName": "job_id",
                    "description": ""
                },
                {
                    "columnName": "department_id",
                    "description": ""
                }
            ]
        },
        {
            "database": null,
            "tableName": "departments",
            "columns": [
                {
                    "columnName": "department_id",
                    "description": ""
                },
                {
                    "columnName": "department_name",
                    "description": ""
                },
                {
                    "columnName": "manager_id",
                    "description": ""
                },
                {
                    "columnName": "location_id",
                    "description": ""
                }
            ]
        },
        {
            "database": null,
            "tableName": "locations",
            "columns": [
                {
                    "columnName": "location_id",
                    "description": ""
                },
                {
                    "columnName": "street_address",
                    "description": ""
                },
                {
                    "columnName": "postal_code",
                    "description": ""
                },
                {
                    "columnName": "city",
                    "description": ""
                },
                {
                    "columnName": "state_province",
                    "description": ""
                },
                {
                    "columnName": "country_id",
                    "description": ""
                }
            ]
        },
        {
            "database": null,
            "tableName": "countries",
            "columns": [
                {
                    "columnName": "country_id",
                    "description": ""
                },
                {
                    "columnName": "country_name",
                    "description": ""
                },
                {
                    "columnName": "region_id",
                    "description": ""
                }
            ]
        },
        {
            "database": null,
            "tableName": "regions",
            "columns": [
                {
                    "columnName": "region_id",
                    "description": ""
                },
                {
                    "columnName": "region_name",
                    "description": ""
                }
            ]
        }
    ]
}