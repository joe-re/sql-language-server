{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Spark Talbes and Functions to sql-language-server Schema File\n",
    "\n",
    "This notebooks creates a simple table with nested columns. It then exports the table and spark functions to a json file which sql-language-server can use to code-complete SQL statement for spark sql.\n",
    "\n",
    "This notebook serves as an illustration of how to do this conversion. This code will eventually be packaged into a pip install so that it can be reused more easily.\n",
    "\n",
    "This code could also be leveraged by the %%sparksql (sparksql-magic) which would make it transparent to the user. Since generation of the schema takes a few seconds, %%sparksql could detect how up-to-date the schema file is and re-generated it only when it passes a certain threshold (say 15min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "DROP TABLE IF EXISTS student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "CREATE TABLE student (\n",
    "    id INT, \n",
    "    name STRING,\n",
    "    `nestedwithspaces` STRUCT<`sub field`:STRUCT<`sub field2`:STRING>>,\n",
    "    age INT,\n",
    "    books ARRAY<STRUCT<`title`:STRING, `chapters`:ARRAY<STRUCT<`paragraph`:STRING>>>>,\n",
    "    struct_col STRUCT<`address`:STRUCT<`streetName`:STRING, `streetNumber`:BIGINT>>,\n",
    "    map_col MAP<STRING, MAP<STRING, STRUCT<`start`:BIGINT,`end`:BIGINT>>>\n",
    "\n",
    "    ) USING PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">database</td><td style=\"font-weight: bold\">tableName</td><td style=\"font-weight: bold\">isTemporary</td></tr><tr><td>default</td><td>student</td><td>False</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "/*\n",
    "    We should have a student table in our database.    \n",
    "*/\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">col_name</td><td style=\"font-weight: bold\">data_type</td><td style=\"font-weight: bold\">comment</td></tr><tr><td>id</td><td>int</td><td>null</td></tr><tr><td>name</td><td>string</td><td>null</td></tr><tr><td>nestedwithspaces</td><td>struct&lt;sub field:struct&lt;sub field2:string&gt;&gt;</td><td>null</td></tr><tr><td>age</td><td>int</td><td>null</td></tr><tr><td>books</td><td>array&lt;struct&lt;title:string,chapters:array&lt;struct&lt;paragraph:string&gt;&gt;&gt;&gt;</td><td>null</td></tr><tr><td>struct_col</td><td>struct&lt;address:struct&lt;streetName:string,streetNumber:bigint&gt;&gt;</td><td>null</td></tr><tr><td>map_col</td><td>map&lt;string,map&lt;string,struct&lt;start:bigint,end:bigint&gt;&gt;&gt;</td><td>null</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "/*\n",
    "    We can see the column nestedwithspaces contains sub-fields with spaces.\n",
    "    Also books is an array of structs and map_col is of MapType.\n",
    "*/\n",
    "DESCRIBE TABLE student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Schema File for sql-language-server\n",
    "Normally the spark catalog would point to a data warehouse full of schemas and tables. In this demo we only have a single student table which we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import json\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTypeName(t):\n",
    "    if type(t) == LongType:\n",
    "        return 'long'\n",
    "    if type(t) == IntegerType:\n",
    "        return 'integer'\n",
    "    if type(t) == StringType:\n",
    "        return 'string'\n",
    "    if type(t) == ArrayType:\n",
    "        return 'array'\n",
    "    if type(t) == StructType:\n",
    "        return 'struct'\n",
    "    if type(t) == MapType:\n",
    "        return 'map'\n",
    "\n",
    "def getPath(path, name):\n",
    "    if ' ' in name:\n",
    "        name = '`' + name + '`'\n",
    "    if len(path) > 0:\n",
    "        return f'{path}.{name}'\n",
    "    return name\n",
    "\n",
    "def getChildren(field, path, fields):\n",
    "    if type(field) == StructField:\n",
    "        getChildren(field.dataType, getPath(path, field.name), fields)\n",
    "    elif type(field) == MapType:\n",
    "        getChildren(field.valueType, getPath(path, 'key'), fields)\n",
    "    elif type(field) == ArrayType:\n",
    "        getChildren(field.elementType, path, fields)\n",
    "    elif type(field) == StructType:\n",
    "        for name in field.fieldNames():\n",
    "            child = field[name]\n",
    "            fields.append({\n",
    "                'columnName': getPath(path, name),\n",
    "                'metadata': child.metadata, \n",
    "                'type': getTypeName(child.dataType),\n",
    "                'description': getTypeName(child.dataType)\n",
    "            })\n",
    "            getChildren(child, path, fields)\n",
    "\n",
    "def getColumns(name):\n",
    "    fields = []\n",
    "    getChildren(spark.table(name).schema, '', fields)\n",
    "    return fields\n",
    "\n",
    "def getTables(database):\n",
    "    rows = spark.sql(f'SHOW TABLES IN {database}').collect()\n",
    "    return list(map(lambda r: {\n",
    "        \"tableName\": r.tableName,\n",
    "        \"columns\": getColumns(r.tableName), \n",
    "        \"database\": None\n",
    "    }, rows))\n",
    "\n",
    "def getDescription(name):\n",
    "    rows = spark.sql(f'DESCRIBE FUNCTION EXTENDED {name}').collect()\n",
    "    textLines = list(map(lambda r: r.function_desc, rows))\n",
    "    return \"\\n\".join(textLines)\n",
    "\n",
    "def getFunctions():\n",
    "    rows = spark.sql('SHOW FUNCTIONS').collect()\n",
    "    return list(map(lambda f: {\n",
    "        \"name\": f.function, \n",
    "        \"description\": getDescription(f.function)\n",
    "    }, rows))\n",
    "\n",
    "def getSparkDatabaseSchema():\n",
    "    return {\n",
    "        \"tables\": getTables('default'), \n",
    "        \"functions\": getFunctions()\n",
    "    }\n",
    "\n",
    "sparkdb_schema = getSparkDatabaseSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save schema to disk. sql-language-server will pickup any changes to this file.\n",
    "outputFileName = '/tmp/sparkdb.schema.json'\n",
    "with open(outputFileName, 'w') as fout:\n",
    "    json.dump(sparkdb_schema, fout, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
